module: cuda	philox engine inputs should also round increment to a multiple of  
module: cuda	 libtorch  put tensor from cpu to gpu is very slow
module: cuda	cudnn status execution failed when using amp
module: cuda	assertionerror  torch not compiled with cuda enabled
module: cuda	model to  cuda   slow takes forever on the new a    gpu
module: cuda	cuda crash when allocating memory 
module: cuda	 bug  torch flip  indexerror for dims    on cuda but works on cpu
module: cuda	move to non legacy magma v  headers
module: cuda	runtimeerror  unrecognized tensor type id  autogradcuda
module: cuda	attributeerror  module  torch  c  has no attribute   cuda memorystats 
module: cuda	 docs  add docstring in torch cuda get device properties
module: cuda	fix nvcc function signature causing assert in typeindex h
module: cuda	program throws exception when using syncbatchnorm with track running stats   false
module: cuda	 replication pad d  raising  cuda error  invalid configuration argument  on large inputs
module: cuda	error with nvidia graphic driver        
module: cuda	an error gpu memery cost in rtx     with few batch size
module: cuda	disable cudnn persistent rnn on sm    devices
module: cuda	matmul won t check operand device and cause wired bugs
module: cuda	subprocess calledprocesserror  command    cmake      build           target    install      config    release           j         returned non zero exit status   
module: cuda	tensor print format issue on nvidia jetson devices
module: cuda	torch  c  cuda init   runtimeerror  cuda unknown error   this may be due to an incorrectly set up environment  e g  changing env variable cuda visible devices after program start  setting the available devices to be zero 
module: cuda	return torch  c  cuda getdevicecount       cuda is not available
module: cuda	 fix  indexing cu  move call to c   cuda kernel launch check to make it reachable
module: cuda	if test suite triggers cuda assert  should stop running tests
module: cuda	 libtorch  tensor to torch  device torch  kcpu   is very slow
module: cuda	 reland   cuda graphs  make cudageneratorimpl capturable  ci all edition 
module: cuda	 libtorch  cuda error  an illegal memory access was encountered
module: cuda	error for c   cuda extension build 
module: cuda	 runtimeerror  cuda error  invalid configuration argument  when operating on some gpu tensors
module: cuda	 windows  torch slogdet returns cuda error when shape      x   
module: cuda	v      block while model cuda  
module: cuda	internal assert failed for maximum
module: cuda	replace kernel resource strings with real  cu source files
module: cuda	 feature  allow user to specify a fraction of the gpu memory  
module: cuda	pinned memory  non blocking feature doesn t seem to work 
module: cuda	inconsistent environment variable naming for setting nvtoolext home in torchconfig cmake
module: cuda	linker error when building from source with cuda     
module: cuda	add cuda      
module: cuda	replace kernel resource strings with real  cu source files
module: cuda	runtimeerror  nvrtc  error  invalid value for   gpu architecture   arch 
module: cuda	cuda out of memory while increase num workers in dataloader 
module: cuda	pytorch doesn t build with cuda     
module: cuda	cudaextension  nvcc does not pick right gcc by default
module: cuda	cuda      compilation flags incorrect for extensions
module: cuda	add tf   information to reproducibility page
module: cuda	dropout broken on cuda for discontiguous channels last input
module: cuda	torch trace type promotion behavior is different on cpu vs cuda
module: cuda	runtimeerror  cuda error  an illegal memory access was encountered
module: cuda	add nondeterministic alert to index copy  median cuda and kthvalue cuda
module: cuda	 nvfuser benchmark minor update
module: cuda	no kernel image is available for execution on the device
module: cuda	torch inverse based on cusolver does not raise error for singular input
module: cuda	quantization observer  proper behavior in multi gpu setting
module: cuda	call to  torch cuda memory stats  before any allocating any tensors on gpu causes segfault
module: cuda	warnings during compiling  floating point value does not fit in required integral type
module: cuda	delete cudaunaryops cpp
module: cuda	integrating cudnn api for lstms with projections
module: cuda	weight variable get device   returns   
module: cuda	cuda problem with linux kernel    
module: cuda	rtx      create cuda tensors much slower after importing transformers library
module: cuda	torch matmul output contains some nan value for large size fp   tensors in v    gpu
module: cuda	kthvalue incorrect with strided gpu tensor
module: cuda	add lazynvrtc
module: cuda	the reproducibility note in  scatter add   documentation is misleading
module: cuda	cuda crash when backproping through a reference to a transposed matrix
module: cuda	inconsistent as tensor converts device when given a dtype
module: cuda	failed to initialize nvml  driver library version mismatch after cuda out of memory error in pytorch
module: cuda	 cuda   function takes too much time with lastest pytorch cuda     
module: cuda	conv transpose d with output padding backpropagation does not work
module: cuda	a error about cuda runtime error    
module: cuda	cuda bfloat embedding
module: cuda	cuda bfloat softmax
module: cuda	cuda bfloat pooling
module: cuda	cuda bfloat activations  
module: cuda	no kernel image is available for execution on the device
module: cuda	cuda bfloat   unary ops part  
module: cuda	cuda bfloat   unary ops part  
module: cuda	embedding max norm wrong results on cuda when repeating indices are present
module: cuda	skip im col vol col when conv kernel is  x 
module: cuda	convert num kernels to int   before calling into cuda get blocks
module: cuda	getting runtimeerror  cuda error  device side assert triggered after using cross enthropy  jupyter nbotebook 
module: cuda	wrong indexing behavior cpu vs cuda
module: cuda	 nccl  record futurenccl callback stream on cuda caching allocator
module: cuda	integer overflow when doing  x  convolution on very large tensor
module: cuda	crossentropyloss does not raise target out of bounds error on gpu
module: cuda	 fix  torch multinomial   fix for   size dim
module: cuda	multinomial distribution of empty tensor crashes when sampling with replacement on cuda
module: cuda	device mismatch in matrix multiplication leads to  runtimeerror  cuda error  an illegal memory access was encountered 
module: cuda	ubuntu      install pytorch support gpu from sources
module: cuda	enable lerp on half type  fix output memory format
module: cuda	 torch  ordereddict key  value   item  operator  const torch  ordereddict std  string  at  tensor   item     cannot be referenced    it is a deleted function
module: cuda	get runtime error when running torch nn conv d forward  
module: cuda	increase the memory requirement of test reduction split
module: cuda	in copy weights to flat buf views   explicitly construct tuple
module: cuda	i cannot use the pytorch that was built successfully from source   dll  initialization routine failed  error loading caffe  detectron ops gpu dll  and previous pkgs are lost 
module: cuda	torch triangular solve gives incorrect results  cpu  for inconsistent systems and no warnings  cpu cuda  
module: cuda	importing pytorch won t load cuda symbols unless forced by torch use rtld global
module: cuda	torch inverse   will cause runtimeerror  cuda runtime error       invalid configuration argument
module: cuda	failed to build with cuda    on windows when system protobuf is used   ambiguous operator overload 
module: cuda	add nvtx range   context manager
module: cuda	keyerror when accessing    cuda array interface    for complex tensors
module: cuda	no indexerror on cuda tensor with out of range indices
module: cuda	pytorch with cuda               what s the best in most situations  
module: cuda	lstmcell and grucell need autocast patching
module: cuda	cuda error in batchnorm
module: cuda	nccl alltoall process group introducing time out of other nccl tests
module: cuda	cuda out of memory in subprocesses spawned by unit tests in windows
module: cuda	module h       error  a member with an in class initializer must be const
module: cuda	einsum fails in thcudablas dgemmstridedbatched only when running on gpu
module: cuda	aten looking for function cusparsedcsrmm  in cuda    header  windows
module: cuda	unclear behavior of the torch cummax function on cuda
module: cuda	remove vs     workaround for autocasting
module: cuda	sparse softmax support  cuda 
module: cuda	cudaerrorillegaladdress printing result of torch nn linear       cuda   torch tensor          
module: cuda	how to enable cudnn tensor op math for float   torch mm operator 
module: cuda	torch inverse   performing very poorly on gpu vs cpu
module: cuda	weird result multiplying a cpu tensor by a cuda   tensor 
module: cuda	cuda runtime error         an illegal memory access was encountered
module: cuda	hide cudnn symbols in libtorch cuda so when statically linking cudnn
module: cuda	fix    bit indexing in gridsampler
module: cuda	do not generate dynamic casting kernel if the op does not support it
module: cuda	segfault in torch cuda comm scatter
module: cuda	torch seed   fails after copying to cuda
module: cuda	cudnn  version check fails
module: cuda	reland add non deterministic alert to cuda operations that use  atomicadd   
module: cuda	einsum running    x slower than expected when controlling for number of ops
module: cuda	pytorch memory footprint much larger than tf onnx 
module: cuda	error in test suite  an illegal memory access was encountered
module: cuda	determinant computation is slow on cuda for singular matrices
module: cuda	system memory leak with mixed precision training
module: cuda	 bug  torch linspace returns different results on cpu vs gpu
module: cuda	runtimeerror  cuda out of memory
module: cuda	crossencropyloss   bug  it can not detect wrong label id on gpu side
module: cuda	missing aten native cuda headers 
module: cuda	reverts running clang tidy on aten
module: cuda	aten looking for function cusparsedcsrmm  in cuda        header but this is no more
module: cuda	cusparse library not found in cuda installation
module: cuda	multi gpus training loss backward    runtimeerror  cudnn error  cudnn status execution failed
module: cuda	 torch cuda set rng state all   documentation refers  new state  as an argument  
module: cuda	torch cuda amp deployed with ddp model meets memory leak
module: cuda	overflow when binary shifting negative integer tensors stored on cpu
module: cuda	incorrect computation of bceloss on cuda when tensor sizes are different
module: cuda	cuda out of memory   but there is enough memory 
module: cuda	cuda illegal memory access error
module: cuda	gpu tensor to  cpu   non blocking true  is blocking
module: cuda	add out  variants for cuda comm broadcast gather scatter
module: cuda	deadlock with shared cuda tensors and multiprocessing  spawn 
module: cuda	build libtorch cxx   abi shared with deps       cu   zip   creat a tensor in cpu is ok  but transfer to gpu error
module: cuda	test argminmax large axis cuda is broken
module: cuda	nn fractionalmaxpooling on cuda does not propagate nans  and does not work for  inf filled tensors
module: cuda	stops cross device data movement in tensor iterator
module: cuda	nearest neighbor upsampling regression
module: cuda	memory leak after oom  maybe rrelu specific 
module: cuda	normal icdf is differs on different cuda devices
module: cuda	test stream event nogil fails on my devfair
module: cuda	potential bug when using fancy indexing with cuda
module: cuda	max pool d creates illegal memory access for large kernel sizes
module: cuda	add index number to thargcheck error message
module: cuda	runtimeerror  cuda error  an illegal memory access was encountered
module: cuda	clip grad norm  fails when parameters are on different devices      regression 
module: cuda	torch  c  cuda getdriverversion   reporting cuda version instead of nvidia driver version
module: cuda	decouple dataparallel distributeddataparallel from cuda 
module: cuda	cuda irfft may be doing unnecessary cloning of input
module: cuda	segment fault after model inference all images using c   api
module: cuda	bug when applying all gather in different group
module: cuda	median with float    lot of nans  and dim   returns random results in cuda
module: cuda	hardsigmoid cuda dispatch ptr internal assert failed
module: cuda	torch cuda amp autocast not working with torchvision models detection maskrcnn
module: cuda	exception  operator  conv d  lost channels last property
module: cuda	grouped convolutions giving different output than separate convs on windows    with cuda
module: cuda	remove thcudamemgetinfo  use c   s cacheinfo instead 
module: cuda	torch multinomial behaves abnormally with cuda tensor
module: cuda	torch cuda is available   get false on cuda  
module: cuda	vectorize loads for reduction kernels when reducing on slow striding dimesions
module: cuda	cannot torch save replicated modules with v   
module: cuda	nightly failed buiding on osx with cuda because of tuple undefined
module: cuda	retire torch cuda nccl py
module: cuda	debug rocm
module: cuda	rip cuda       thc
module: cuda	rip cuda       circleci  aten  and caffe 
module: cuda	update cusparse deprecated xcsrmm  call
module: cuda	makes cuda  float  uint  cast consistent with cpu
module: cuda	regression with explicit type conversion from float to uint  on gpu devices 
module: cuda	allow torch cuda amp gradscaler to support sparse gradients
module: cuda	bypass  getdevicefromptr  check when device is known
module: cuda	non blocking syncbatchnorm update
module: cuda	from dlpack multiprocess cuda error
module: cuda	move the record stream to aten operation
module: cuda	torch cuda amp gradscaler  runtimeerror
module: cuda	runtimeerror  cowardly refusing to serialize non leaf tensor which requires grad  since autograd does not support crossing process boundaries
module: cuda	runtimeerror for randn on cuda
module: cuda	intermittent segfaults during libtorch shutdown   proposed fix
module: cuda	runtimeerror  cuda runtime error         device side assert triggered at  pytorch aten src thc thcgeneral cpp    
module: cuda	indexerror  dimension out of range  expected to be in range of          but got    
module: cuda	lack of amp support for sparse gradients
module: cuda	cuda runtime error         system not yet initialized     thcgeneral cpp   
module: cuda	torch topk gives incorrect results with float  
module: cuda	lshift and rshift on cuda should match the behavior on cpu
module: cuda	torch load map location  cuda   always indicates device 
module: cuda	 runtimeerror  copy      cuda error  unspecified launch failure
module: cuda	support nhwc upsampling for cuda 
module: cuda	grid sample returns corrupted tensor when grid max   and grid min   has huge difference
module: cuda	 reland   eager autocasting  out of place ops only  with msvc      fix 
module: cuda	 autocast  alternative workaround so autocast can compile with buggy visual studio     
module: cuda	torch cat is moving tensors across devices silently
module: cuda	add s workaround for scalartype  byte for cuda
module: cuda	 device count    returns   while  torch  c  cuda getdevicecount    returns   
module: cuda	make gpu loops support mutable lambda
module: cuda	 avg pool d  needs to be converted to support  densenet     in channels last mode
module: cuda	make dynamic casting case also benefit from unrolling
module: cuda	make discontiguous tensors also benefit from unrolling
module: cuda	multidimensional cuda irfft modifies input
module: cuda	tensor  copy should not copy when src and dst point to the same memory
module: cuda	help     runtimeerror  fractional max pool d backward out cuda failed with error code  
module: cuda	floating point exception  core dumped 
module: cuda	topk backward twice  the second input is nan  to trigger cuda assert 
module: cuda	cuda s  dropout  doesn t support channels last memory format
module: cuda	cuda s  cat  implementation doesn t support memory format
module: cuda	 max pool d  fails with channels last and specific shapes 
module: cuda	debug build over cuda      always fails
module: cuda	max pool d doesn t check if input is contiguous  max pool d with indices out cuda frame failed with error code   with specific inputs
module: cuda	segfault upon calling torch tensor on a gpu tensor
module: cuda	random    is not supported for bfloat   cuda tensors on windows
module: cuda	why pytorch      python  is slower than pytorch      python 
module: cuda	torch tril indices returns a float tensor on master
module: cuda	torch rfft returns nans for some half precision cuda inputs
module: cuda	 do not merge c   host device
module: cuda	merge minor cases of gpu loops
module: cuda	kill cudadeviceallocator in thcstate
module: cuda	issue with longe sequence
module: cuda	pytorch cuda extensions break during compilation with cuda       
module: cuda	pytorch with cuda     
module: cuda	cuda out of memory when trying to allocate    eib
module: cuda	installation issue
module: cuda	cuda error  an illegal memory access was encountered when using output padding in nn convtranspose d
module: cuda	device   has index   calls torch cpu function and crashes
module: cuda	incorrect result of argmax on cuda with large non contig tensors and on non contig dimensions
module: cuda	lstm can t work with nn dataparallel
module: cuda	bizarre  no kernel image  error for pytorch built from source
module: cuda	when using autograd in a jupyter notebook  cuda is initialized even if cuda is not used
module: cuda	magmainitializescorrectly cuda tries to take the inverse of a singular tensor
module: cuda	pytorch     bug in conv d kernel size  x  runtimeerror  cudnn error  cudnn status not supported  this error may appear if you passed in a non contiguous input
module: cuda	cuda mps system
module: cuda	very slow moving model to device with model to device 
module: cuda	creating tensors and moving them to cuda in subprocess fails   hangs forever
module: cuda	linspace logspace support all integral types on cuda but not cpu
module: cuda	number of workers and batch size multiplication problem
module: cuda	what should i do to disable libcaffe  nvrtc so
module: cuda	tensoriterator unrolling and vectorized load for gpu loop
module: cuda	strange memory behavior when moving module to cuda
module: cuda	segmentation fault in  torch  autograd  engine  evaluate function   in      
module: cuda	model to  cuda   get large memory
module: cuda	gpu is not recognized  gcp image  c  deeplearning common cu            
module: cuda	how to set cuda stream by  call aten function
module: cuda	index put  does no longer work with int   tensor and cuda
module: cuda	test conv transposed large cuda failed on windows
module: cuda	the example program using libtorch is not linked against torch cuda when use cuda is on
module: cuda	cudnn convolution memory usage
module: cuda	torch no grad   context manager seems to leak memory
module: cuda	nll loss doesn t support empty tensors on gpu
module: cuda	sparse cuda inplace multiplication seems to give incorrect results
module: cuda	we should test   cuda array interface   stride handling
module: cuda	power  p    node pytorch compilation from source with cuda       bus error   out of memory
module: cuda	should we expose circleci cuda   tests as ximportant and run on prs 
module: cuda	matrix multiplication returns wrong result when middle dimension is  
module: cuda	runtimeerror  parallel for failed  cudaerrornokernelimagefordevice with v      and a tesla k  c
module: cuda	runtimeerror  cublas runtime error when using torch bmm on cuda   
module: cuda	compile error for  cu files when using libtorch  caused by nvcc flags 
module: cuda	cuda error  device side assert triggered for bert multi class classification
module: cuda	cannot init cuda under cuda       pytorch      
module: cuda	test int pow cuda failed on windows
module: cuda	pin memory stuck in ddp reducer constructor
module: cuda	nccl error    invalid argument
module: cuda	wrong result for cuda   implementation of  m   addmv  m            for fp   on some old architecture
module: cuda	index put  lack device check for accumulate true  resulting cuda backend crash
module: cuda	testautograddevicetypecuda test to sparse cuda doesn t work
module: cuda	pdist can t handle batch         
module: cuda	running fp   softmax on a fp   tensor for dimensions other than the last fails during the backward pass
module: cuda	why when i use torch cuda empty cache    it cost some gpu memory on other device 
module: cuda	fp   fails to speed up hrnet
module: cuda	missing cuda kernel operator
module: cuda	 pytorch     cuda    driver        not available
module: cuda	request  oneapi support
module: cuda	get  runtimeerror  cuda error  device side assert triggered 
module: cuda	how to release gpu  memory  of intermediate result tensor
module: cuda	cudnn batchnorm has batch size limit for eval with channel
module: cuda	thassert thctensor  checkgpu  state     self   src   src    
module: cuda	windows release of pytorch     is missing several required cuda libraries
module: cuda	cusparse handle is not thread safe
module: cuda	findcuda  use find program instead of find path to find nvcc
module: cuda	uncoalesced add dense  sparse  missing sparse values for torch int   on cuda
module: cuda	migrate frac from th to aten  cuda 
module: cuda	 cuda memcheck  testtensordeviceopscuda test topk dim desc sort cuda float   fails
module: cuda	 cuda memcheck  testtorchdevicetypecuda test kthvalue cuda float   fails
module: cuda	channels last max pool d fails with specific inputs
module: cuda	set cuda device is expensive operation
module: cuda	argmax for half datatype 
module: cuda	remove tools setup helpers cuda py 
module: cuda	make tensoriterator stop promoting types by copying
module: cuda	cuda unavailable when pytorch         installed with cudatoolkit     
module: cuda	github action clang tidy report error on cuda related files
module: cuda	inconsistent handling of   sized dims in  distributions categorical  between cpu and gpu
module: cuda	cuda model   takes too much time to load on pytorch    
module: cuda	install cuda for clang tidy
module: cuda	 easy  fix most documentation warnings
module: cuda	torch random cuda documentation missing
module: cuda	fix padding idx in the new embedding cuda kernel 
module: cuda	quantizationed model cannot inference with cuda 
module: cuda	 rocm  add bfloat   support in linear algebra on rocm
module: cuda	torch histc added a finite range check to resolve segfaults if tensor has inf  also added checks for nan values  min max
module: cuda	 don t close  docs ci        docs
module: cuda	multi gpu gather is much slower than scatter
module: cuda	migrate soft margin loss from the th to aten  cuda cpu 
module: cuda	cpu strided complex support for reduce ops and linpack ops
module: cuda	add fused layer norm impl on cuda in pytorch
module: cuda	optimize layernorm on cuda
module: cuda	out variant for torch batch norm elemt
module: cuda	free memory after cuda out of memory error
module: cuda	removes floating dtype decorator from test torch and test cuda
module: cuda	clone   changed to clone at  memoryformat  contiguous 
module: cuda	 wip  help needed   d sep convolution 
module: cuda	at  nogradguard no grad
module: cuda	nonzero performance improvement
module: cuda	fix native ctc loss gradient indexing bug for large target sizes
module: cuda	use torch check instead of at assert in torch  cuda  gather  
module: cuda	stops common utils py from setting the default tensor type  to torch doubletensor 
module: cuda	ctcloss cuda backend computes wrong gradient when target  i e  label  length is greater than     for double inputs or      for float inputs
module: cuda	move hipify to torch utils to bundle them into torch package
module: cuda	wrapping namespace reduction in namespace at         
module: cuda	check dataptr s deleter to determine if it is allocated by cuda in record stream
module: cuda	 rocm  add some support for the occupancy api on rocm
module: cuda	move the cuda implementation of sqrt to aten 
module: cuda	record stream   for shifted view tensors
module: cuda	record stream   on a shifted view tensor doesn t work
module: cuda	comprehensive ish instrumentation for cuda memory allocator
module: cuda	fix to operate on cuda kernel with clang and libc           
module: cuda	fix to operate on cuda kernel with clang and libc           
module: cuda	fix to operate on cuda kernel with clang and libc           
module: cuda	fix to operate on cuda kernel with clang and libc           
module: cuda	remove cuda tensor apply 
module: cuda	 wip  v      quantization cherrypicks
module: cuda	enabled bfloat   for cuda
module: cuda	let logical xor support non bool tensors  again 
module: cuda	build extensions with without cuda
module: cuda	support   batch size for nn linear 
module: cuda	makes test cuda py s generated tensor op tests generic
module: cuda	 torch cdist  raises cuda error on backward with too big batch
module: cuda	migrate max and min  binary  from th to aten 
module: cuda	makes more of test cuda py generic and updates test torch tests
module: cuda	ensure nccl error handling code is disabled for nccl versions      
module: cuda	runtimeerror  cuda error  device side assert triggered
module: cuda	rtx      conv d fails with benchmark true
module: cuda	serialize xla tensor
module: cuda	migrate le gt ge eq ne from the th to aten  added support of type promotion 
module: cuda	migrate le gt ge eq ne from the th to aten  added support of type promotion 
module: cuda	uniform random generator generates too many zeros compared to numpy
module: cuda	kill more unused thcunn operators 
module: cuda	kill a number of unused thcunn operators 
module: cuda	make named tensor implementations more robust
module: cuda	migrate ne from the th to aten
module: cuda	migrate eq from the th to aten
module: cuda	migrate le from the th to aten
module: cuda	migrate ge from the th to aten
module: cuda	make repeat respect the current stream
module: cuda	migrate gt from the th to aten
module: cuda	move the cuda implementation of log p to aten 
module: cuda	migrate gt from the th to aten
module: cuda	named tensor support for  index fill   index fill  squeeze  median tensor 
module: cuda	expose piecewiselineartransform to pytorch
module: cuda	 wip  clone empty supports memory format simplified
module: cuda	test cuda kernel loop overflow large fails on current build
module: cuda	make resize as  generic  so xla works 
module: cuda	port l  loss to aten
module: cuda	migrate  cosh  and  cosh   from th to aten  cuda 
module: cuda	move the cuda implementation of log  to aten 
module: cuda	adds device generic precision tests to test torch py
module: cuda	pytorch  try using rtld local instead of rtld global
module: cuda	move the cuda implementation of log   to aten 
module: cuda	creates torch precision tests  moves more of test cuda py to test torch py
module: cuda	port gatedlinearunit forward to tensoriterator 
module: cuda	 test do not merge  test for cuda    
module: cuda	 jit python none should have its type inferred as nonetype
module: cuda	fixing windows build
module: cuda	port cuda sigmoid to aten cuda 
module: cuda	makes test indexing py device generic
module: cuda	per channel fake quant
module: cuda	aten port of lgamma  cuda 
module: cuda	port cuda implementation of expm  to aten
module: cuda	named tensor support for logsumexp  mode  kthvalue  median  min  max
module: cuda	 test do not merge  cuda      test  again
module: cuda	 torch zeros  produces error   runtimeerror  cuda runtime error      
module: cuda	port mse lose to aten
module: cuda	gradient scaling api
module: cuda	implement multiple dispatch         
module: cuda	move the cuda implementation of log to aten 
module: cuda	implement resize   resize as  for named tensors
module: cuda	migrate multinomial from the th to aten  cuda 
module: cuda	implement multiple dispatch
module: cuda	back out   pytorch  pr  fix many type mismatches in the cuda version of calc digamma and calc trigamma 
module: cuda	moves more tests to testtorchdevicetype
module: cuda	enabled where for bool tensor on cuda
module: cuda	enabled bfloat   dtype on cuda
module: cuda	puts rocm tests on default stream
module: cuda	removes torchtest  expands generic device testing
module: cuda	 restack  kill if true   if false in declarations cwrap 
module: cuda	fix ctc loss argument check error message
module: cuda	fix named tensor masked fill  on cuda
module: cuda	 unstack  replace simple if true   if false cases in declarations cwrap 
module: cuda	vectorize unary operators lgamma and erfinv  do not merge 
module: cuda	fix cdist gradient computation if first arg is  xn
module: cuda	back out   pytorch  pr  refines test torch py generic device testing 
module: cuda	refines test torch py generic device testing
module: cuda	ports most of test torch py to generic device type framework
module: cuda	adds sync to flaky test events multi gpu query
module: cuda	allow batch size of   in conv
module: cuda	port fused layer norm from apex to aten
module: cuda	add type hint for cuda set rng state
module: cuda	rewrite argmax and argmin as tensoriterator reductions
module: cuda	move the cuda implementation of atan   which was partially implemented in aten  to aten 
module: cuda	enabled bfloat   dtype on cuda
module: cuda	 test  skip testing triangular solve batched on non default cuda stream
module: cuda	skip test triangular solve batched
module: cuda	fix remaining invalid function cast warnings that show up with gcc    
module: cuda	disable test cuda test stream event nogil on rocm
module: cuda	turn on build namedtensor permanently
module: cuda	 rocm  enable more mgpu tests
module: cuda	run test cuda py under cuda memcheck in ci
module: cuda	add type promotion support in sparse tensors
module: cuda	 c    projects fail to build with libtorch nightly   cuda     
module: cuda	migrate lt and lt  from the th to aten
module: cuda	enables  do cuda non default stream
module: cuda	creates generic device type testing framework
module: cuda	 rocm  enable unit tests
module: cuda	 rocm  make regular softmax warp size aware
module: cuda	update torchscript docs
module: cuda	 rocm  make persistent softmax warp size aware 
module: cuda	add setup metadata to help pypi flesh out content on pypi package page
module: cuda	 rocm  make lookup table warp size aware
module: cuda	 rocm  make spatial depthwise convolution warp size aware
module: cuda	 rocm  make sparse coalesce warp size aware
module: cuda	unify treatment of warp size   wave size
module: cuda	 rocm  remove null arguments that have been marked deprecated by rocblas
module: cuda	move cuda abs to aten
module: cuda	throw runtime exception for integer div by zero
module: cuda	fix cudnn build error with cc    platform        
module: cuda	ignore f    in all   init   py without putting noqa
module: cuda	fix many type mismatches in the cuda version of calc digamma and calc trigamma
module: cuda	fix race in cuda initialization
module: cuda	spell out the legacy  cuda th  functions that are used 
module: cuda	fix int   overflow in summaryops cu getbin        
module: cuda	refactor torch  solve tests
module: cuda	 rocm  rocblas deprecated the last two parameters 
module: cuda	move most build namedtensor macros out of header areas
module: cuda	pytorch       produce  unspecified launch failure  on mac os using cuda 
module: cuda	add op bitwise xor to replace   xor   and   ixor  
module: cuda	migrate digamma and polygamma from the th to aten  cuda 
module: cuda	 rocm   wip  with the update to rocm      we can now hipify occupancy 
module: cuda	rename packed tensor accessor
module: cuda	implement multiple dispatch
module: cuda	switch to rocthrust for thrust cub apis         
module: cuda	kill if true   if false in declarations cwarp 
module: cuda	replace simple if true   if false cases in declarations cwrap 
module: cuda	kill aten custom call 
module: cuda	stop reordering th random function arguments 
module: cuda	stop re ordering th c blas arguments 
module: cuda	switch to rocthrust for thrust cub apis         
module: cuda	make torch checks same for both cpu and cuda multinomial
module: cuda	name inference for masked fill    masked fill
module: cuda	name inference rule for masked select
module: cuda	reimplemented spatialdepthwiseconvolutionupdateoutput
module: cuda	reimplemented spatialdepthwiseconvolutionupdateoutput
module: cuda	fix to operate on cuda kernel with clang and libc  
module: cuda	fix cuda distributions test on windows
module: cuda	runtimeerror  cuda error  device side assert triggered   yesterday it worked
module: cuda	eliminate magic numbers in batchlinearalgebra cu
module: cuda	migrate pow from th to aten  cuda 
module: cuda	libtorch torch  cat fails with c   memory exception after upgrading to     stable
module: cuda	fix implicit fallthrough warnings in featurelppooling cu
module: cuda	adding  d depthwise convolution
module: cuda	move the cuda implementation of trunc to aten 
module: cuda	run clang format on torch lib c  d
module: cuda	move the cuda implementation of floor to aten 
module: cuda	in the cuda implementation of erfinv  erfinv   should be used for double
module: cuda	delete totype const deprecatedtypeproperties       
module: cuda	revert  add logical xor operator 
module: cuda	fixed masking warnings in tests
module: cuda	cuda ipc garbage collection hangs when disposing of lstms 
module: cuda	test branch
module: cuda	move the cuda implementation of rsqrt to aten 
module: cuda	fix dependency by moving dimname  h cpp  namedtensor  h cpp  to core 
module: cuda	revert  let logical xor support non bool tensors  
module: cuda	move build namedtensor macro out of header areas
module: cuda	split binaryopskernel cu into a file per kernel 
module: cuda	replace open registration tensortypeid with closed enum 
module: cuda	torch bmm produces incorrect results for gpu fp   tensors with batch size        
module: cuda	creates torch friendly event class and adds stream tracking to autograd
module: cuda	implement name inference for torch bmm
module: cuda	 do not review  channels last perf test
module: cuda	automatic mixed precision for pytorch
module: cuda	implement bool tensor bernoulli 
module: cuda	move the cuda implementation of round to aten 
module: cuda	cuda   method warning in pycharm
module: cuda	extend tensoriterator to support dim apply
module: cuda	 migrate erfinv and erfinv  from the th to aten  cuda 
module: cuda	 bc breaking  add align corners option to grid sample and affine grid  change default to false
module: cuda	embedding backward pass taking     x longer than forward pass
module: cuda	port unfold from th to aten
module: cuda	enable gradient scaling in autograd
module: cuda	remove thchalfautonumerics cuh
module: cuda	deprecate tensor data t     and codemod tensor data t    to tensor data ptr t   
module: cuda	max pool d cuda should have channel last optimized kernels performance improvement 
module: cuda	 rocm  use correct warp size for rocm for embeddingbag
module: cuda	move the cuda implementation of ceil to aten 
module: cuda	remove unused thtensor  add  and similar functions code 
module: cuda	migrate gather into aten
module: cuda	 draft  do not review  port cpu pow from th to aten
module: cuda	 rocm  for int   t atomicadd  use the available compiler builtin on rocm 
module: cuda	 do not review  port pow from th to aten
module: cuda	port pow from th to aten 
module: cuda	cuda kernel loop  prevent int overflow in loop increment 
module: cuda	thcnumerics cuh error when i use pytorch cuda port to compile
module: cuda	implement name inference for torch dot
module: cuda	implement name inference for addmv  addmv   mv
module: cuda	 jit  misc doc updates   
module: cuda	enable torch cholesky for batches         
module: cuda	port fmod from th to aten
module: cuda	torch cholesky throws cuda error  invalid configuration argument
module: cuda	 nhwc support for adaptive avg pool d   adaptive avg pool d backward 
module: cuda	implement operators logical and and logical or
module: cuda	 do not merge  v      libtorch binary build
module: cuda	speed up binary half operators on cuda with computing capacity       
module: cuda	cdist allocates a huge amount of memory in the bachward pass  pytorch       
module: cuda	enable broadcasting of batch dimensions rhs and lhs tensors for lu solve
module: cuda	port fmod from th to aten
module: cuda	illegal memory access occurs when using nn avgpool d 
module: cuda	implement name inference for mm  addmm  addmm 
module: cuda	lazy initialization of cuda context to allow forking and independent cuda context creation 
module: cuda	max pool d cuda should have channel last optimized kernels
module: cuda	windows build not working  crash after start
module: cuda	allow torch tril   triu to handle bool and half inputs
module: cuda	cuda gdb pytorch crash
module: cuda	 do not merge  debug binary linux conda     cpu devtoolset  build
module: cuda	 wip  scatter gather memory format
module: cuda	port addcdiv operator from the th code to aten
module: cuda	conv d conv d performance on tesla arch   half precision 
module: cuda	tensoriterator  binary op input output overlap check
module: cuda	 wip   add tensor iterator and some cuda functions memory propagation
module: cuda	 cublas runtime error  for  not so large   fp    matrix multiplication 
module: cuda	 v       fix regression in triangular solve when number of batches     for cuda
module: cuda	sumop for int  
module: cuda	let logical xor support non bool tensors 
module: cuda	fix expansion of stride argument
module: cuda	fix expansion of stride argument in avg pool d
module: cuda	fix expansion of stride argument in avg pool d
module: cuda	fix expansion of stride argument in max pool d
module: cuda	 v      retry  fixed bool in isintegraltype bug  plus review comments 
module: cuda	fix expansion of stride argument in max pool d
module: cuda	fix regression in triangular solve when number of batches     for cuda
module: cuda	 retry  fixed bool in isintegraltype bug  plus review comments 
module: cuda	 bc breaking  add align corners option to grid sample and affine grid  change default to false
module: cuda	fixed bool in isintegraltype bug  plus review comments 
module: cuda	let logical not support non bool tensors 
module: cuda	 rocm  fix test testcuda test streams multi gpu query
module: cuda	 empty like    to    resize as   and  clone  now preserve memory format
module: cuda	 quantization  clang format aten src aten native quantized
module: cuda	fixed bool in isintegraltype bug
module: cuda	add logical xor operator
module: cuda	add logical not operator 
module: cuda	max pool d with indices cuda and max pool d with indices backward cuda should have channel last optimized kernels 
module: cuda	move addcmul to aten cuda 
module: cuda	support gather different indices for different examples in one batch         
module: cuda	updated docs and added deprecation warnings to acknowledge a bool ten 
module: cuda	rename previously thnn conv kernels to have naive  prefix 
module: cuda	changed tensor comparison return type from uint  to bool         
module: cuda	jetson tx  crashes when running simple nn on gpu using libtorch
module: cuda	documentation for tensor record stream  
module: cuda	error  no functions are registered for schema aten  empty    
module: cuda	        fix ctc loss for zero length targets on gpu         
module: cuda	add aligned option to roialign
module: cuda	add tests to ensure that both abs      and abs       lead to    
module: cuda	port addcdiv operator from the th code to aten
module: cuda	 wip  revert  changed tensor comparison return type from uint  to bool      
module: cuda	 v       allowing batching for det logdet slogdet operations         
module: cuda	migrate neg s cuda implementation to aten 
module: cuda	 don t merge  don t close  v         stable docs job
module: cuda	fix regression in torch qr         
module: cuda	fix regression in torch qr
module: cuda	at  view         
module: cuda	fix regression in torch qr
module: cuda	add scalar support for scatter add 
module: cuda	compilation error cuda      clang      
module: cuda	port atan  from th to aten
module: cuda	fix gemm call for cudablas for thcunn conv        
module: cuda	maxpool d makes model not reproducible 
module: cuda	add nn functional interpolate slow down backward process more than    times
module: cuda	refactor randperm test
module: cuda	optimize sparse adam by fusing kernels
module: cuda	 rocm  fix testcuda test events wait
module: cuda	 add torch check to disable sub for bool tensors
module: cuda	improvements on scatter and gather  prepare for implementing broadcasting
module: cuda	port  pow  operator from the th code to aten
module: cuda	pin memory should not copy on already pinned tensors
module: cuda	allow scatter and gather to broadcast
module: cuda	rename gels to lstsq
module: cuda	 fix  at  view
module: cuda	let set rng state and get rng state accept string parameter
module: cuda	 wip  disabled minus for bool
module: cuda	make torch view drop all names on named tensors
module: cuda	creates torch friendly event class   adds stream tracking to autograd
module: cuda	runtimeerror  cuda runtime error       initialization error after daemonization
module: cuda	remove unused cublas driver functions for getrs
module: cuda	enabled cumsum and cumprod for bool tensors
module: cuda	support torch  tensor and at  tensor with bool and bfloat   dtypes 
module: cuda	add torch check to disable sub for bool tensors
module: cuda	 wip  port torch eq from th to aten
module: cuda	fix error message for a wrong fork cuda
module: cuda	fix ctc loss for zero length targets on gpu
module: cuda	at  view         
module: cuda	enable addition for bool on cuda 
module: cuda	enable addition for bool on cuda 
module: cuda	support gather different indices for different examples in one batch
module: cuda	 fix  at  view
module: cuda	re land  fix error message for a wrong fork cuda 
module: cuda	open source  d depthwise conv
module: cuda	fix for cdist backward for non batch tensors  do not review 
module: cuda	max pool with indices  return valid indices if all input elements are  inf
module: cuda	remove empty thcthreadlocal  h  cpp 
module: cuda	fix indexing for more than       elems in non indexed first dim
module: cuda	enabled masked select out to work with bool masks
module: cuda	revert  improve performance of advanced indexing backward          
module: cuda	 rocm  update rocm ci to python   
module: cuda	update scatterweightedsum op
module: cuda	added checks for the output sizes in fractionalmaxpool d and fractionalmaxpool d
module: cuda	libtorch cannot find cuda
module: cuda	improve enforce in cross entroy op
module: cuda	pytorch keeps running unknown processes in other gpus
module: cuda	enabled  add cuda  for bool and fixed alpha scalar bug
module: cuda	make randperm work properly on non contiguous tensors 
module: cuda	remove cuda free mutex
module: cuda	versioned nightly url for libtorch with cuda      doesn t work
module: cuda	fix error message for a wrong fork cuda
module: cuda	port  resize as   and  clone  from th to aten
module: cuda	add missing comment from       
module: cuda	named inference rules for some initializer fns
module: cuda	moved at  assert no internal overlap to tensoriterator
module: cuda	fix for cdist backward for non batch tensors
module: cuda	allowing batching for det logdet slogdet operations
module: cuda	 rocm   rocm      enable jit fusion on rocm
module: cuda	moving sign function to aten
module: cuda	zero gradients beyond a certain buffer size on cuda
module: cuda	 fixing reduction kernel launch 
module: cuda	port  sign  operator from the th code to aten
module: cuda	port  remainder  operator from the th code to aten
module: cuda	port  pow  operator from the th code to aten
module: cuda	port  erfinv  operator from the th code to aten
module: cuda	port  digamma  operator from the th code to aten
module: cuda	port  clamp  operator from the th code to aten
module: cuda	port  atan   operator from the th code to aten
module: cuda	port  addcmul  operator from the th code to aten
module: cuda	port  addcdiv  operator from the th code to aten
module: cuda	back out  back out   pytorch  pr  move thnvrtc and dynamiclibrary to aten  
module: cuda	add fill diagonal function         
module: cuda	move the body of fill kernel impl into fill kernel cuda
module: cuda	move fill and fill diagonal to fill cpp  fill h  and fillkernel  cpp cu 
module: cuda	initiate checkcuda  bug warning
module: cuda	add support for non affine batch norm with float stats and half inputs
module: cuda	back out   pytorch  pr  move thnvrtc and dynamiclibrary to aten 
module: cuda	avoid potential extra copy in  lu with info cuda
module: cuda	 reland  update note about tensors on cpu for certain magma functions  elimina 
module: cuda	cuda optimization of thctensormathpointwise cuh
module: cuda	 wip  bfloat   for cuda
module: cuda	 upsample nearest 
module: cuda	 wip  channels propagation prober old code
module: cuda	pytorch creating  st tensor slow
module: cuda	segmentation fault for  import torch  on ubuntu          prime select intel
module: cuda	simplify the flow control in div kernel cuda 
module: cuda	fix torch normal with cuda tensors
module: cuda	runtimeerror  cuda error  invalid configuration argument
module: cuda	change tensoriterator to be stack allocated  using named return value optimization to elide copies 
module: cuda	conv d fails with bigger batch size
module: cuda	missing some optional flags for the cuda extensions
module: cuda	set stream for softmax kernel launch
module: cuda	torch nn functional gumbel softmax yields nans
module: cuda	avg pool d avg pool d for longtensor
module: cuda	enable test torch py tests for bfloat   on cuda
module: cuda	numerical stability of embedding kernels
module: cuda	port lu solve to aten
module: cuda	move thnvrtc and dynamiclibrary to aten
module: cuda	move thnvrtc and dynamiclibrary to aten
module: cuda	rename macros and build options namedtensor enabled to build namedtensor
module: cuda	auto differentiating torch cdist is broken on gpu
module: cuda	 wip   ignore  bfloat   for cuda
module: cuda	remove trailing semicolon from torch check macros 
module: cuda	 rocm  update rocm ci to python   
module: cuda	add a bitwise not operator for integer and boolean types  cuda  
module: cuda	more named inference rules for pointwise unary ops
module: cuda	use integer floor division for pooling shape computation
module: cuda	performance improvements for depthwise convolutions in fp  
module: cuda	optimize instancenormgradientop
module: cuda	add totalglobalmemory in cudacachingallocator
module: cuda	remove unused param in caffe  layernormgradientop
module: cuda	improve handling of mixed type tensor operations
module: cuda	named inference rule for more pointwise ops 
module: cuda	enable named inference for some unary pointwise ops
module: cuda	updated docs and added deprecation warnings to acknowledge a bool tensor
module: cuda	fix syncbatchnorm running var update issue
module: cuda	lu  when not using pivoting  return the identity permutation instead of zeros
module: cuda	fix bug in caffe  transpose on gpu
module: cuda	pin memory malloc now uses existing context if available 
module: cuda	fixes bugs in torch multinomial without replacement
module: cuda	fix race condition  bad lock hierarchy  move getfreemutex   into autoncclgroup 
module: cuda	named inference rule for  abs  
module: cuda	optimize instancenormop forward
module: cuda	building from source failed  multiple errors in the printout
module: cuda	 rocm  fix dispatching of backwards kernel for rocm 
module: cuda	refactor and improve randperm tests 
module: cuda	swap detection order in randperm out cuda to avoid unnecessary conversion from float when the input is small 
module: cuda	support half type in randperm 
module: cuda	 rocm  first round of optimizations for segment reduction op kernels 
module: cuda	 rocm  sparse blas  remove workaround to check zero length inputs 
module: cuda	runtimeerror  cudnn error  cudnn status internal error in       
module: cuda	support half type for randperm 
module: cuda	warn on conditions that can trigger cublas sgemm bug
module: cuda	tentatively remove special treatment of half type in randperm out cuda
module: cuda	use at  detail    instead of detail    to avoid ambiguity in windows
module: cuda	fix  error   detail is ambiguous  on windows
module: cuda	porting convtranspose d to aten
module: cuda	optimization of the embedding and embedding bag cuda kernel
module: cuda	 jit  clean up old uses of checkscript
module: cuda	return device object instead of device index for torch cuda current device
module: cuda	pytorch should provide cuda     binaries 
module: cuda	enable promotion of tensor types in tensoriterator operations 
module: cuda	remove type dispatch
module: cuda	move backward and set data off of type
module: cuda	cuda error       invalid device ordinal
module: cuda	runtimeerror  cuda error       invalid device ordinal
module: cuda	remove getdevicefromptr and allocator from type
module: cuda	enabled gather and scatter for bool tensor
module: cuda	segmentation fault using all reduce with cuda    mpi 
module: cuda	updating upsampling bilinear d kernel 
module: cuda	remove most usages of thchalfautonumerics 
module: cuda	in place operation on differentiable view leaks memory
module: cuda	always enable p p access for gpu copies
module: cuda	 rocm   rocm      switch to rocthrust for thrust cub apis
module: cuda	ctcloss cuda backward throws setstorage  storage size mismatch error
module: cuda	port symeig to aten and enable batching of inputs
module: cuda	runtimeerror  cuda error  an illegal memory access was encountered
module: cuda	quantized conv avoid functional usage
module: cuda	turn off non default stream testing 
module: cuda	enabled mul for bool tensors on cuda
module: cuda	port im col and vol col
module: cuda	 qat  add fakequantize module
module: cuda	tune elementwise ops for rocm
module: cuda	revert  torch rename          
module: cuda	 wip  draft  channels last operators
module: cuda	upsample nearest cuda kernel update
module: cuda	allow batch sizes         for inverse  solve  cholesky solve and tria 
module: cuda	fix spelling errors
module: cuda	with emit nvtx context manager is broken in current master 
module: cuda	fix flaky nuclear norm   test
module: cuda	make range functions respect current stream
module: cuda	 testing  enable stream switching on test advancedindex
module: cuda	 testing  enable stream switching on all tests 
module: cuda	skip triangular solve cuda test on non default stream
module: cuda	some parts of pytorch test suite don t work properly on non default stream
module: cuda	port svd to aten  enable batching for matrix inputs
module: cuda	test triangular solve is flaky
module: cuda	imported target  torch  includes non existent path  c  program files nvidia corporation nvtoolsext include 
module: cuda	 cuda  refactor random number generators in aten
module: cuda	 codemod  at check    torch check
module: cuda	enabled bfloat   storage
module: cuda	cuda implementation of alias multinomial doesn t work correctly
module: cuda	don t leak threads on exit
module: cuda	replace nullary unary binary loops with generic implementation
module: cuda	change pytorch tests to use non default cuda stream
module: nn	the computation of weighted crossentropyloss is not reasonable
module: nn	cleaned up moduleattributeerror
module: nn	some question about hardsigmoid
module: nn	mypy error when passing none to nn adaptiveavgpool d or nn adaptivemaxpool d
module: nn	add complex support for torch nn l loss
module: nn	how to apply functions to nested modules 
module: nn	 parameters   is empty in forward when using dataparallel
module: nn	allow  nn unflatten  to take a  sequence int   instead of specifically a  tuple int       
module: nn	program throws exception when using syncbatchnorm with track running stats   false
module: nn	the entmax alpha and entmax   feature in contrib
module: nn	 wip  add complex support for torch nn mseloss 
module: nn	newmoduletest  merge  check jacobian  with  check gradgrad 
module: nn	add  text to num head in mathematical formula
module: nn	torch nn adaptiveavgpool d  results in reproduction
module: nn	conv d sometimes produces wrong results on cpu
module: nn	 alexnet  object has no attribute  features 
module: nn	how to do polymorphism on torch  nn  moduleholder 
module: nn	torch nn module named parameters has wrong type annotation
module: nn	gumbel softmax issue in hard mode 
module: nn	gaussian nll loss
module: nn	fix incorrect warnings in parameterlist dict
module: nn	nn utils spectral norm   does not perform normalization of the weight matrix 
module: nn	tf nn raw rnn
module: nn	proper lstm input
module: nn	how to convert syncbn to batchnormnd 
module: nn	add adabelief optimizer to the c   api
module: nn	torch dropout  fix non contiguous layout input
module: nn	torch dropout  avoid unnecessary mul for p  
module: nn	add lazyconvxd and lazyconvtransposexd
module: nn	dropout broken on cuda for discontiguous channels last input
module: nn	constructing a parameterdict raises a warning
module: nn	fix some flaky tests in test torch py and test nn py
module: nn	working as the eval   mode  but the net with the batchnorm layer will raise an error  suggesting needing more than one sample input 
module: nn	batch norm const running mean  var modified in place
module: nn	padding idx and provided weights in nn embedding and nn functional embedding 
module: nn	weight variable get device   returns   
module: nn	groupnorm is slow stragely when calling backward   in pytorch      
module: nn	add note about in place weight modification for nn embedding
module: nn	crossentropyloss hyperlinks don t work
module: nn	about aten src aten native cudnn conv cpp  issue
module: nn	bug in max pool d with ceil mode under certain conditions
module: nn	 native  convtranspose d crashed
module: nn	nn module script
module: nn	a new type of nn module param for large transient data  that doesn t get saved 
module: nn	allow converting parameters of nn module to complex dtypes
module: nn	  call   signature of nn module
module: nn	implementation of  d bicubic grid sampler
module: nn	fixed runtimeerror during the deserialization
module: nn	embedding bag running on cpu produces wrong result when weight tensor is non contiguous 
module: nn	module prune has no effect on speed and memory consumption
module: nn	 nn module      property    wrong traceback message
module: nn	complex linear does not work
module: nn	autocompletion does not work in torch nn module
module: nn	proposal  generic triplet margin loss
module: nn	f mse loss a  b  reduction  elementwise mean   value is incorrect and doesn t show deprecation warning when  nd argument requires gradient
module: nn	nn module forward has no docstring
module: nn	enable torch nn quantized typechecks during ci
module: nn	dropout in nn multiheadattention causing attention weight sum    
module: nn	 d affine transfrom f affine grid 
module: nn	shapes not documented properly in marginrankingloss 
module: nn	fix  torch nn functional grid sample  crashes if  grid  has nans
module: nn	segfault in  torch nn functional grid sample  in reflection mode
module: nn	segmentation fault in adaptive avg pool d
module: nn	linear function and the different convs functions retain unnecessary buffers
module: nn	add inplace option for torch nn hardsigmoid and torch nn hardswish layers
module: nn	wrong in nn parameterlist and nn dataparallel  pytorch   
module: nn	check track running stats if it is not in dict  before get it from self   dict   in extra repr of  normbase
module: nn	accessing state dict   of models trained with previous pytorch version
module: nn	 add padding  same  mode to conv       d  nn conv       d
module: nn	documentation wrong for linear layer initialization
module: nn	fix    bit indexing in gridsampler
module: nn	raise runtimeerror for zero stride pooling
module: nn	segfault in torch nn functional embedding
module: nn	torch nn functional avg pool d has floating point exception when stride is  
module: nn	torch nn functional grid sample segfaults on large inputs
module: nn	hooks registered to weight norm parameters cause runtimeerror
module: nn	torch nn functional layer norm returns nan for fp   all   tensor
module: nn	unflatten module for nn sequential
module: nn	pytorch memory footprint much larger than tf onnx 
module: nn	possible bug in rnn py
module: nn	maxpool layer does not correctly implement padding 
module: nn	added silu activation function
module: nn	move the pruning module to a higher level
module: nn	 mean  reduction result in crossentropyloss mismatches with manually computing mean
module: nn	runtimeerror   th exp out not supported on cudatype for long
module: nn	moduledict documentation misleadingly describes ordering of keys
module: nn	trying to implement dropconnect  wan et  al        in pytorch    
module: nn	nn parameter overrides global gradient settings
module: nn	retain graph   true out of memory issue
module: nn	torch nn utils clip grad clip grad norm  is too slow 
module: nn	nan handling in cuda nn maxpool d
module: nn	add  generator   kwarg for dataloader   random samplers
module: nn	throw error when embeddingbag      sparse true  and slice the embedding s weight
module: nn	use nn spectral norm in lstm
module: nn	unnecessary extra memory allocation when using circular padding
module: nn	replicationpad accepts   dim batch size 
module: nn	add global hooks to  torch nn module 
module: nn	module register backward hook doesn t work
module: nn	max pool d creates illegal memory access for large kernel sizes
module: nn	add tags to torch nn parameter
module: nn	incorrect nn functional interpolate behavior for align corners false
module: nn	implementing hyperlstm
module: nn	hardsigmoid cuda dispatch ptr internal assert failed
module: nn	convert sync batchnorm should respect device affinity
module: nn	 batchnorm  unexpected behaviour with track running stats
module: nn	throw an error instead of warning on softmax implicit dimension
module: nn	inconsistent behavior in model parameters   in pytorch      
module: nn	in place leakyrelu backward calculation is triggered with a non positive slope which is not supported
module: nn	suggestion for unify parameter name in nn conv and nn linear
module: nn	copy deepcopy   breaks when pruning is set on sequential
module: nn	fixed  pad circular indexing bug
module: nn	add support for non persistent buffers 
module: nn	replace pruning hooks in place if possible 
module: nn	incremental pruning reorders module hooks
module: nn	syncbatchnorm size check update
module: nn	misleading documentation in torch nn functional fold
module: nn	under  ceil mode true  for  avgpooling d   pytorch fails in calculating pooling output shape as expected  and gets  nan  results 
module: nn	syncbatchnorm size check
module: nn	c   make namedanymodule name  any  public
module: nn	maxunpool is incorrect
module: nn	multi channel linear layer
module: nn	nn utils rnn pack padded sequence is broken
module: nn	 adaptivemaxpool d  operator export to onnx error
module: nn	nn conv d doesn t handle well padding in the borders of matrix
module: nn	add documentation for featurealphadropout
module: nn	conv d performance improvement
module: nn	torch load map location  cuda   always indicates device 
module: nn	could please add a member function like  nn module get out channels   
module: nn	different gradients with batch norm on gpu and cpu
module: nn	 fixes         fix attributeerror for nn module s properties
module: nn	nn linear with empty tensor backward error  cuda 
module: nn	is there a way to initalize the weights of nn convtranspose d by k nearest interpolation using nn init 
module: nn	discrepancy of matrix multiplication due to the size
module: nn	wrong error is raised for property of nn module
module: nn	results of multiheadattention depend on the query length
module: nn	add support for bool byte  attn mask  tensor in multiheadattention transformer modules
module: nn	multilayer birnns are sharing state across directions
module: nn	torch nn utils prune remove prevents future model training
module: nn	rnns do not have gradients while using dataparallel in      
module: nn	may be there is a bug in nn moduledict  
module: nn	runtimeerror in pack sequence
module: nn	dirac init compatibility with group convolutions
module: nn	please add the exception args to the  load from state dict   method 
module: nn	pytorch not reproducible on cpu
module: nn	string formatting 
module: nn	showing symbolic representation
module: nn	tests for verifying behaviour of batchnorm using   dim batch sizes 
module: nn	pytorch       weight drop    lstm  object has no attribute  weight hh l  
module: nn	 v       fix and add more padding mode support for conv         
module: nn	nll loss does not check for negative indices
module: nn	how to implement multiple different kernel shapes in  d convolution 
module: nn	c   batchnorm   instancenorm modules register parameters incorrectly when affine is false
module: nn	torch stft causes segmentation fault with data parellel 
module: nn	why not combine the operation of with torch no grad   into the net eval   
module: nn	python stub files for torch nn seem misconfigured
module: nn	macos        on ryzen get intel mkl error  cpu   is not supported 
module: nn	my program stops at loss backward   without any prompt in cmd 
module: nn	automatically inferring
module: nn	make it an option to have a different mask for each sequence in a batch for the transformer
module: nn	add swa to pytorch mainline
module: nn	remove bad implementation of functional grad
module: nn	nn transformer generate square subsequent mask does not behave as expected
module: nn	thassert thctensor  checkgpu  state     self   src   src    
module: nn	floating point exception and segfault for empty tensors to batchnorm d
module: nn	flipped look ahead attention mask for transformer
module: nn	multi head attention forward give individual mask for each src
module: nn	instancenorm will be converted to syncbn
module: nn	proposal to change the offsets for the embeddingbag operator in pytorch
module: nn	typo in  from  replicate import replicate as repliace 
module: nn	add torch nn gelu as the module for gelu activation
module: nn	improve error message in nn multiheadattention
module: nn	 docs  docs missing online for rnnbase flatten parameters
module: nn	nn avgpoolxd raises error for too small input 
module: nn	poissonnllloss does not compute the value documented
module: nn	none buffer doesn t get set by load state dict
module: nn	adding a  fit   method to nn module
module: nn	transformer generate square subsequent mask
module: nn	torch save cannot find source of modules
module: nn	multiheadattention incorrectly handles mask
module: nn	issue        fix  bug in updating moduledict   parameterdict
module: nn	 easy  fix most documentation warnings
module: nn	 quantization  changing observer name
module: nn	fix padding idx in the new embedding cuda kernel 
module: nn	migrate soft margin loss from the th to aten  cuda cpu 
module: nn	 quantization  refactoring names for consistency
module: nn	fix test overwrite module params on conversion cpu cuda after type promotion introduced for comparison ops         
module: nn	docstr  nn i quant mod  convrelu d and linearrelu
module: nn	docstring on nn intrinsic quantized modules  convrelu d and linearrelu
module: nn	add fused layer norm impl on cuda in pytorch
module: nn	removes  default floating dtype decorator
module: nn	not possible to use different key value dimensionalities in nn multiheadattention
module: nn	torch nn linear       does not support batch size  
module: nn	conv d crashes with  stride   
module: nn	add missing optional annotation 
module: nn	add missing optional annotation 
module: nn	issue        fix  bug in updating moduledict   parameterdict 
module: nn	modify pytorch s integration of nnpack to use a unified underlying thread pool implementation 
module: nn	fix test overwrite module params on conversion cpu cuda after type promotion introduced for comparison ops         
module: nn	adding docstrings for nnq functional
module: nn	fix test overwrite module params on conversion cpu cuda after type promotion introduced for comparison ops         
module: nn	fix test overwrite module params on conversion cpu cuda after type promotion introduced for comparison ops         
module: nn	revert to align corners true as default 
module: nn	make it hard error for grid sample and affine grid
module: nn	fix native ctc loss gradient indexing bug for large target sizes
module: nn	type hints  return  iterator  instead of  iterable  from    iter   
module: nn	stops common utils py from setting the default tensor type  to torch doubletensor 
module: nn	can not update nn moduledict nn parameterdict with another same type dictionary 
module: nn	nn sync bn does not work well with apex fp   mode
module: nn	use nnpack for strided convolutions 
module: nn	 jit  kill  parameter list
module: nn	 jit  kill  parameter list
module: nn	 jit  reduce special casing around  training 
module: nn	switching to profilinggraphexecutor  wip 
module: nn	 quantization  adding docstrings for nnq functional
module: nn	use nnpack for strided convolutions 
module: nn	 pointwise loss appears to be dead code
module: nn	 wip  v      quantization cherrypicks
module: nn	 batchnorm update 
module: nn	 quantization  move quantized  mul scalar to aten
module: nn	 quantization  move quantized  add scalar to aten
module: nn	 quantization  move quantized  mul scalar to aten
module: nn	fixes packedsequence to  and unifies packedsequence conversions 
module: nn	 quantization  move quantized  add scalar to aten
module: nn	nn linear   output is nan for large matrix multiplication 
module: nn	support   batch size for nn linear 
module: nn	 quantization  rename  intrinsic to intrinsic
module: nn	 quantization  fix reprs for  intrinsic modules
module: nn	refactor nn  intrinsic to nn intrinsic 
module: nn	makes more of test nn generic
module: nn	 pytorch  perf  use nnpack for strided convolutions 
module: nn	 jit  reduce special casing around  training 
module: nn	 quantization  improve repr for quantized modules
module: nn	 v       fix test overwrite module params on conversion cpu cuda after comparison ops type promotion was introduced
module: nn	 v       fix test overwrite module params on conversion cpu cuda after comparison ops type promotion was introduced
module: nn	 update init py   support independent random generator without resetting global seed
module: nn	 docs  add docstring to torch nn functional softplus
module: nn	use grad out for cudnn ctc loss
module: nn	 quantization  improve repr for quantized modules
module: nn	autograd  double backwards function for binary cross entropy loss
module: nn	migrate le gt ge eq ne from the th to aten  added support of type promotion 
module: nn	migrate eq from the th to aten
module: nn	eliminate outdated comments
module: nn	fake quantization enhancements for qat ptq support  fix tests
module: nn	fix quantizeavx  build issue 
module: nn	remove fbgemm is cpu supported in favor of torch backends quantized supported qengines
module: nn	 quantization  throw if someone tries to torch save   quantized modules
module: nn	fix ddp incompatibility issue with nn multiheadattention 
module: nn	creates device generic cudnn decorators
module: nn	update onnx export for interpolate in opset   
module: nn	fix linter
module: nn	use noop observer to pass dtype for dynamic quantization
module: nn	multiheadattention and ddp incompatability
module: nn	 jit  module dedupe
module: nn	updates and extends testnndevicetype
module: nn	quantized interpolate kernel upsample bilinear d 
module: nn	quantization aware training  freeze batch norm support
module: nn	quantized interpolate kernel upsample nearest d 
module: nn	support for add relu functional module
module: nn	backward consistent behavior for avgpool d avg pool d in pytorch    
module: nn	skip some fragile tests
module: nn	nn embedding with max norm shows unstable behavior and causes sometimes runtime error 
module: nn	quantized average pool d and adaptive avg pool d implementation revert d         
module: nn	 rename  quantize linear    quantize per tensor
module: nn	 batchnorm num batches tracked update 
module: nn	renames  tensor renamed    rename    tensor names     rename  
module: nn	per channel baseline
module: nn	implement multiple dispatch         
module: nn	implement multiple dispatch
module: nn	 wip  fix   repr   function for logsoftmax and add test
module: nn	expands test nn device generic test class
module: nn	fuse module enhancements
module: nn	removes torchtest  expands generic device testing
module: nn	add noqengine to qengine
module: nn	changes to support int  weight and fp   bias in qnnpack
module: nn	refactor checked tensor unwrap to take devicetype instead of backend
module: nn	fold activation permutation inside quantized conv operator
module: nn	fold weight permutation inside quantized conv operator
module: nn	fix typo in transformerencoder and transformerencoderlayer documentation
module: nn	fix typo in transformerencoder and transformerencoderlayer documentation
module: nn	allow batch size of   in conv
module: nn	port fused layer norm from apex to aten
module: nn	enabled conv methods for the bfloat  
module: nn	multi head attention forward produces nan
module: nn	 re submit  quantization  torchscript serialization for dynamic lstm
module: nn	remove requests as dependency
module: nn	revert  torchscript serialization for dynamic lstm module 
module: nn	 pytorch  migrate away from using legacy variable constructor in test nn py
module: nn	 quant  dynamic quantization for bias 
module: nn	add device check before accessing data ptr in packlayer
module: nn	 rocm  enable more mgpu tests
module: nn	 pt   quant  add the fp   weight support for lstm in dynamic quantize
module: nn	 rocm  enable unit tests
module: nn	 rocm  make regular softmax warp size aware
module: nn	 c   api  l loss module
module: nn	add the quantized average pool d support and adaptive avg pool d support
module: nn	e quantization  torchscript serialization for dynamic lstm module
module: nn	ensure that ddp wrapped module has parameters that require gradients
module: nn	fix for conv shape check prints overflowed ints
module: nn	ignore f    in all   init   py without putting noqa
module: nn	c   average pool module
module: nn	add torch nn identity to   init   pyi in
module: nn	 wip  jit  decorator for compiling functions while tracing
module: nn	 jit  decorator for compiling functions while tracing
module: nn	 jit  decorator for compiling functions while tracing
module: nn	 wip  temp commit to add bias   method
module: nn	backward hook triggered despite removablehandle remove  
module: nn	 jit  add support for moduledict
module: nn	 quantization  rename fbgemm quantized operators to generic quantized ops
module: nn	implement multiple dispatch
module: nn	fix the document of kaiming initialization
module: nn	 quantization  store bias in packedconvweight in fbgemm
module: nn	fix typing on nn parameter
module: nn	adding mish activation function
module: nn	wrong crossentropy loss calculation
module: nn	the math formula of  kaiming uniform   and  kaiming normal   are not general enough 
module: nn	change the method type of generate square subsequent mask
module: nn	runtimeerror  cuda error  device side assert triggered   yesterday it worked
module: nn	illegal instruction  core dumped  when using conv d
module: nn	fix typo in transformerencoder documentation
module: nn	fixes       
module: nn	type annotation for torch nn functional is missing
module: nn	adding  d depthwise convolution
module: nn	 quantization  store bias in packedlinearweight struct in fbgemm
module: nn	test conv api    main   moduleapitest  is flaky
module: nn	update transformer py comments to include a full example
module: nn	allow  d attn mask in multi head attention forward
module: nn	get rid of torch  thnn 
module: nn	remove module  backend as it s not used anymore 
module: nn	move autograd function for crossmaplrn d from being backend specific to modules  functions 
module: nn	 quantization  rename fbgemm quantized operators to generic  quantized  ops
module: nn	move new criterion tests from test nn py to common nn py
module: nn	kill backend specific lookup in crossmaplrn d  as it never succeeds 
module: nn	kill convtransposemixin forward  which doesn t seem to be used 
module: nn	remove thnn sparse autograd functions 
module: nn	kill thnn function auto generation 
module: nn	kill thnn function auto generation 
module: nn	no double backwards implemented for several rnn ops
module: nn	add python c   api parity tracker for torch nn
module: nn	documentation for cdist
module: nn	serialization for nn quantized functional modules
module: nn	work around for bias quantization for conv and linear operators
module: nn	 quant  adding return for the observer in the functional modules py
module: nn	ignore index does not have expected behavior
module: nn	 quant  pt   add the dynamic quantized lstm module
module: nn	 do not review  channels last perf test
module: nn	torch nn relu forced inplace true
module: nn	fix the lint error in transformer doc 
module: nn	correct the type comment of  pad   
module: nn	issue         fix cuda method to support  none  arg for device and a  
module: nn	 onnx  momentum setting in syncbatchnorm forward  inference  pass 
module: nn	change mask rcnn backbone 
module: nn	 docs  added documentation for nn functional bilinear 
module: nn	bcelosswithlogits input     bceloss sigmoid input  
module: nn	 bc breaking  add align corners option to grid sample and affine grid  change default to false
module: nn	serialization for nn quantized functional modules
module: nn	embedding backward pass taking     x longer than forward pass
module: nn	simplify some code
module: nn	fix typing error for padding with asymmetric signatures
module: nn	resolve the atten weight making nan in multiheadattention forward func
module: nn	update affine grid docs with suport for  d affines
module: nn	max pool d cuda should have channel last optimized kernels performance improvement 
module: nn	c   maxpool module
module: nn	fix deprecation warnings
module: nn	describe the relation between fold and unfold operations 
module: nn	fixed error in transformer example
module: nn	remove unused files from thnn and thcunn
module: nn	modify the sublayer connection in transformer  module
module: nn	transformer encoder layer with src key padding makes nan
module: nn	allow syncbatchnorm without ddp in inference mode
module: nn	update onnx export for interpolate in opset   
module: nn	 quant  added relu  kernel
module: nn	 quant  use absolute import of the parent folder without alias 
module: nn	work around for bias quantization for conv and linear operators
module: nn	circular padding doesn t work when the  right  side is  
module: nn	change kernel size to self kernel size to resolve error in quantized conv module
module: nn	mask type in nn transformer
module: nn	added  pyi file for flatten
module: nn	enable log softmax and crossentropyloss for bfloat  
module: nn	 quant  adding scalar add mul 
module: nn	 quant  adding quantized mul kernel
module: nn	 quantization  extra repr for quantized modules
module: nn	 quantization  assert weight observer has the correct dtype
module: nn	 fix  convert bias to float in quantized conv module
module: nn	adds a placeholder for the  mul  operator 
module: nn	add  pair for quantized conv module
module: nn	support load state dict func for nn multiheadattention
module: nn	 nhwc support for adaptive avg pool d   adaptive avg pool d backward 
module: nn	enables  inplace  in the quantized relu
module: nn	regarding hierarchy
module: nn	 quantization  test    init   from float  on nnq  d  linear
module: nn	 quant  observer  change return type of observer to two tensors
module: nn	fix fan out calculation for depthwise convolution
module: nn	illegal memory access occurs when using nn avgpool d 
module: nn	max pool d cuda should have channel last optimized kernels
module: nn	 quantization  fix and test conv d constructor and from float
module: nn	make flake  complain about trailing whitespace
module: nn	 pt   quant  add the default weight observer for the dynamic quantization path
module: nn	 quant  initial version of pytorch graph mode quantization
module: nn	 quant  addrelu module
module: nn	 quantization  fix incorrect type annotation on linear   setstate  
module: nn	 quantization  fix py  imports in  intrinsic modules
module: nn	 jit  support multiheadedattention module
module: nn	extend nn transformer to support bert  gelu 
module: nn	incorrect conv d result
module: nn	c   fold nn module
module: nn	replacing axis with dim in quantized cat
module: nn	fixed disconnected mmaps in multi head attn forward
module: nn	 quantization  jit serialization for conv d
module: nn	 quantization  state dict serialization for conv d   some bugfixes
module: nn	 quantization  re work conv d
module: nn	added type annotations to unpooling layers
module: nn	 quantization  state dict serialization of nnq linear
module: nn	 quantization  simplified nnq linear class
module: nn	pruning functionality
module: nn	resolve unused variables in tests
module: nn	 quantization  jit serialization of nnq linear
module: nn	 quantization  state dict serialization of nnq linear
module: nn	 quantization  simplified nnq linear class
module: nn	fix qconv benchmark
module: nn	pytorch   
module: nn	print padding mode for conv modules if not zeros
module: nn	fix scale and zero point names
module: nn	cleanup torch nn functional py
module: nn	add support to serialize quantized module in jit
module: nn	 bc breaking  add align corners option to grid sample and affine grid  change default to false
module: nn	 quantization  jit script   testing and fixes
module: nn	 cudnn nhwc support 
module: nn	enable oss quantization tests         
module: nn	add python c   torch nn api parity test harness
module: nn	cosineembeddingloss
module: nn	properly mangle  nn module   construct 
module: nn	 quantization  enable oss quantization tests
module: nn	 quant  add intrinsic module mappings
module: nn	exporting weights from  torch nn lstmcell 
module: nn	error  no functions are registered for schema aten  empty    
module: nn	 wip  quantization  enable oss quantization tests
module: nn	        fix ctc loss for zero length targets on gpu         
module: nn	 wip  maybe fix qat import
module: nn	maybe fix qat import
module: nn	        fix align corners doc
module: nn	fix align corners doc
module: nn	enabling inplace in quantized relu
module: nn	 jit  support nn gru and make nn lstm accept packedsequence
module: nn	 quantization  fix conv d
module: nn	 jit  don t try to set training after scriptmodule has been initialized 
module: nn	 jit  don t try to set training after scriptmodule has been initialized 
module: nn	 jit  make nn lstm accept packedsequence instead of tuples
module: nn	removing the make module script 
module: nn	use protected  float module class variable
module: nn	 don t merge  don t close  v         stable docs job
module: nn	 quant  qat modules take qconfig as argument and keep qconfig as member
module: nn	conv d   conv d register backward hook inconsistency
module: nn	enable fbgemm tests under ubsan as well
module: nn	 wip  jit  skip ci onnx fix
module: nn	fix gemm call for cudablas for thcunn conv        
module: nn	maxpool d makes model not reproducible 
module: nn	port  pow  operator from the th code to aten
module: nn	 quant  add nnq identity
module: nn	add float module to quantized conv
module: nn	qconv relu and qlinear relu modules
module: nn	 fix  add type checks in  intrinsics modules fused
module: nn	 wip  add support to serialize quantized module in jit
module: nn	 qat  convbn d convbnrelu d
module: nn	fused qconv d relu kernel
module: nn	 feature request  inferred module dimensions
module: nn	add notes about overshoot in bicubic mode
module: nn	 feature request  gabor layer
module: nn	model eval   with ensemble of cnn is not extended to the sub models 
module: nn	fix ctc loss for zero length targets on gpu
module: nn	make module  first version
module: nn	optimizing out the division in the fusion
module: nn	 jit  support nn gru in script
module: nn	 jit  support pack padded sequence and pad packed sequence
module: nn	 fix  add type checks in  intrinsics modules fused
module: nn	 fix  quantized conv module
module: nn	 fix  nn quantized test
module: nn	fix for cdist backward for non batch tensors  do not review 
module: nn	max pool with indices  return valid indices if all input elements are  inf
module: nn	documentation cleanup
module: nn	add document of functions nn init ones  zeros 
module: nn	 wip  quantized average pool kernel
module: nn	 pt   quant  dynamic quantized linear module
module: nn	 quant  fix qat tests
module: nn	moving np function to test area
module: nn	 wip  ignore  cleanup in quantized
module: nn	 rocm  update rocm ci to python   
module: nn	added checks for the output sizes in fractionalmaxpool d and fractionalmaxpool d
module: nn	 quant  add fused modules in nn  intrinsic
module: nn	 qat  conv module
module: nn	 qat  conv module
module: nn	 reland  qat  quantization aware training in eager mode
module: nn	debug mseloss  wrong document
module: nn	pytorch       conv d with dilation     crashes on cpu  but runs perfectly on gpu
module: nn	 fix  conv module missing return
module: nn	conv d multichannel broken
module: nn	remove hard dependency on numpy from quantized conv 
module: nn	 qat  convbn d convbnrelu d
module: nn	 qat  linearrelu module
module: nn	pytorch runtimeerror  expected stride to be a single integer value or a list of   values to match the convolution dimensions  but got stride       
module: nn	 qat  convrelu d module
module: nn	 wip  fusion and  intrinsic modules
module: nn	 quant  add fused modules in nn  intrinsic
module: nn	invert ownership between pyfunction and thpfunction 
module: nn	fix for ceil mode pooling size calculation
module: nn	 jit  support pack padded sequence and as tensor
module: nn	 doc  fix f one hot doc signature
module: nn	remove usage of legacy autograd function
module: nn	add support to print qtensor in cpp
module: nn	 qat  conv module
module: nn	qconv operator level benchmark
module: nn	 pt   quant  dynamic quantized linear operator and module
module: nn	add better performing versions for groupwise and depthwise convolutions
module: nn	remove distributeddataparallelcpu as ddp now supports cpu models
module: nn	 wip  nestedtensor prototype
module: nn	remove rtti check for tensorimpl shadow copy
module: nn	maxpool d in the torch
module: nn	 bc breaking  remove legacy autograd function
module: nn	add support for non affine batch norm with float stats and half inputs
module: nn	fix lint
module: nn	quantized conv avoid functional usage
module: nn	 qat  quantization aware training in eager mode
module: nn	v   initial commit
module: nn	 qat  add nn qat linear
module: nn	implements geometric mean initialization
module: nn	fix a fixme in test nn
module: nn	dataparallel issue with torch spmm 
module: nn	error when loading a sparse tensor parameter from a state dict in pytorch      
module: nn	 upsample nearest 
module: nn	added generation of transpose and dilated  d and  d for longtensor
module: nn	 wip  channels propagation prober old code
module: nn	add key padding mask kwarg to transformer
module: nn	add module requires grad 
module: nn	keep reuqires grad unchanged after converting bn to syncbn
module: nn	fix spectral norm load state dict with strict false
module: nn	syncbatchnorm test mode
module: nn	fix bug  add batch first arg on  pack sequence   
module: nn	runtimeerror  cuda error  invalid configuration argument
module: nn	proposal for the gumbel softmax stability issue
module: nn	avg pool d avg pool d for longtensor
module: nn	add type stubs to import  nn  modules
module: nn	 pt   quant  add the support of handle bias being nullptr for torch ops quantized fbgemm linear
module: nn	add the support of handle bias being nullptr for torch ops quantized fbgemm linear
module: nn	only scatter in forward if multi device per process
module: nn	add key padding mask argument to transformer module
module: nn	remove  vf and use torch directly
module: nn	remove unused attr from embeddingbag docs 
module: nn	 lint  fix lint in torch nn quantized modules linear py
module: nn	use linear operator with fp   weights in jit
module: nn	 rocm  update rocm ci to python   
module: nn	interpolate in the  bicubic  mode with the same shape  transpose the tensor 
module: nn	use integer floor division for pooling shape computation
module: nn	fix lint issues
module: nn	 c    can t use sequential inside sequential 
module: nn	support data parallel for heterogeneous gpu
module: nn	fix two overindent lint errors in test common nn py 
module: nn	 module  add mutation support for forward pre hook and forward hook
module: nn	packedsequence s sorted indices is not put on cuda when to  cuda   is called 
module: nn	fix syncbatchnorm running var update issue
module: nn	added a flatten module
module: nn	 jit  remove weak script
module: nn	 jit  special case nn modules functions for recursive script
module: nn	remove weak script annotations
module: nn	fix doc about rnn weight ih l k  shape
module: nn	sync batchnorm running var update issue
module: nn	update on   jit  special case nn modules functions for recursive script 
module: nn	remove weak script annotations
module: nn	revise error message for invalid reduction
module: nn	improve repr for incompatiblekeys
module: nn	 easy  restore default values on premature test exit
module: nn	make dropout   repr   consistent with other modules
module: nn	fix minor issues with       
module: nn	runtimeerror  cudnn error  cudnn status internal error in       
module: nn	 jit  script  invert permutation  in utils rnn
module: nn	support sparse gradients in distributeddataparallel
module: nn	support linear operation with fp   weights in aten
module: nn	porting convtranspose d to aten
module: nn	optimization of the embedding and embedding bag cuda kernel
module: nn	make  nn sequential  forward function more flexible
module: nn	torch quantization conversion utilities  observers for eager mode quantization
module: nn	use latest stable flake  bugbear in ci and fix b    flake  error 
module: nn	remove many usages of type
module: nn	unexpected output size for maxpool
module: nn	split nn module  save to state dict to make it overridable
module: nn	nn quantized relu and nn quantize quantize dequantize modules
module: nn	 quant  add serialization for nn quantized linear module
module: nn	implementation of nn quantized linear module
module: nn	quick fix for         the cpu case
module: nn	 bc breaking  use fn param  instead of fn param data  in nn module  apply
module: nn	changing behavior of im col and col im to be inverses of each other 
module: nn	quantized conv avoid functional usage
module: nn	 jit  fix constant batch size in rnns
module: nn	 qat  add fakequantize module
module: nn	backend py   getattr   must raise attributeerror
module: nn	add edit docs for nn transformer
module: nn	add error message to missing function backend
module: nn	support accumulating ddp grads using a context manager
module: nn	port avg pool d   to aten
module: nn	port avg pool d   avg pool d cpu implement to aten
module: nn	 wip  draft  channels last operators
module: nn	define   setstate   on  convnd to handle pre padding mode pickles 
module: nn	disable nondeterministic ctcloss from cudnn
module: nn	fix         fix numerical issue in log softmax
module: nn	fix spelling errors
module: nn	fix multihead attention for half
module: nn	port avg pool d   to aten
module: nn	add  torch   future    overwrite module params on conversion  global flag  and check it in  nn module  apply   
module: nn	adding the quantized namespace to torch nn and importing it from torch
module: nn	 testing  enable stream switching on all tests 
module: nn	update module py
module: nn	 please ignore  testing  
module: nn	 please ignore  testing
module: nn	change pytorch tests to use non default cuda stream
module: nn	pack padded sequence  check for empty  zero element  tensors
module: nn	onnx export interpolate  resize  for opset version   
module: nn	revert  revert d           pytorch  pr  continuation of port max unpool d  max unpool d and max unpool d to aten 
module: nn	fix embedding bag nan output when input is empty
module: nn	gumbel softmax arxiv docs link fix
module: nn	modify gelu impl with tensoriterator
module: nn	quantized conv d module
module: nn	cast dropout to float in rnn
module: nn	update multiheadattention module support key value with different number of features and allow static key value
module: nn	move thctensor  normal  normal means  normal stddevs  normal means stddevs  to aten
module: nn	fix gelu documents
module: nn	retry fix python dataparallel rnn in no grad mode
module: nn	 spectral normalization  keyerror on load state dict
module: nn	add gelu gradient for pytorch
module: nn	 pytorch  pr  functional conv d
module: nn	fix python dataparallel rnn in no grad mode
module: nn	 jit  make scriptmodule training an attribute instead of a parameter
module: nn	 jit  fix   constants   for some nn modules
module: nn	fix model to xla device 
module: nn	fix latex formular error about  normal
module: nn	fix syncbatchnorm doc
module: nn	immutable batchnorm options
module: nn	porting spatialdilatedconvolution and volumetricdilatedconvolution to aten
module: nn	ctc loss  fix backward when  d target tensor is larger than max target length
module: nn	remove  convtransposemixin forward  
module: nn	fix align corner docs
module: nn	fix subscripts in rnn documentation
module: nn	 quant  add torch nn  intrinsic and torch nn  intrinsic quantized namespace
module: nn	dilatedmaxpool d  small cleanup
module: nn	port dilated max pool d   to aten
module: nn	use a simpler way to delete recursive function
module: nn	revert  lightweight at most once logging for api usage          
module: nn	latex of init norm is wrong
module: nn	question about embed  backend embedding apply
module: nn	fix tests for multi head attention 
module: nn	remove padding mode from torch nn functional conv       d s docstr
module: nn	batchnorm d implementation returns different results than expected
module: nn	fix annotation for optional broadcastinglistx 
module: autograd	torch  autograd  python  pythonengine  thread init crash at exit
module: autograd	count nonzero  autograd internal assert failure
module: autograd	can you add higher order derivative support for torch s embedding function  
module: autograd	 feature request  hessian support for model weights  nn parameter 
module: autograd	gradgradcheck for torch repeat and torch tile is outrageously slow
module: autograd	runtimeerror  unrecognized tensor type id  autogradcuda
module: autograd	some question about hardsigmoid
module: autograd	torch autograd jacobian returns tensors with all zeros
module: autograd	rocm ci is intermittently failing with std  out of range
module: autograd	newmoduletest  merge  check jacobian  with  check gradgrad 
module: autograd	use with torch no grad   has memory leakage issue with certain conditions
module: autograd	view tracking for autograd should not save optional std  function
module: autograd	strides computed via autograd grad vs those computed by backward   can be different
module: autograd	python pickle serialization mangles nlllossbackward objects in tensor objects
module: autograd	problem with nested gradient calculation
module: autograd	internal assert failed for  s unique   shape 
module: autograd	core dump when checking that basic cnn works  python     
module: autograd	in place view functions might not actually be views
module: autograd	a problem about elu backward
module: autograd	pytorch autograd gil error in deployed flask app
module: autograd	update retain grad message to be less confusing
module: autograd	add  inputs  argument to  autograd backward   
module: autograd	assigning a parameter to an indexed tensor that was produced by ddp no longer works in torch nightly      
module: autograd	calculate partial derivatives instead of total derivative like tf gradients with parameter stop gradients 
module: autograd	torch angle   grads are wrong
module: autograd	exception type changed when it s raised in backward operation
module: autograd	run gradgradcheck on torch fft transforms
module: autograd	avoid mat  references in mm mat  backward
module: autograd	cpu memory leak when using torch no grad  
module: autograd	fixed handling of nan for evenly distribute backward
module: autograd	tripletmarginwithdistanceloss example code doesn t work correctly
module: autograd	backward of repeat   crash if repeated size is  
module: autograd	bucketize fails with inputs requiring gradient
module: autograd	conv transpose d with output padding backpropagation does not work
module: autograd	 emit nvtx  example in docs seems to be broken 
module: autograd	failing gradcheck for  d adaptive pooling functions
module: autograd	test autograd test fails with rocm
module: autograd	enable grad decorator doesn t support recursive calls
module: autograd	doc  bad rendering of  tensor  output 
module: autograd	allow  tuple  of  tensor  to be passed to  autograd function 
module: autograd	runtimeerror  function  mmbackward  returned nan values in its  th output 
module: autograd	multivariatenormal backprop performance issue related to broadcasting
module: autograd	numerical instability in divbackward  for multiple backward
module: autograd	uninformative forward trace in detect anomaly for double backward
module: autograd	autograd not working for torch exp  j   phase 
module: autograd	derivative for floor divide is not implemented
module: autograd	torch norm gives nan gradient when i input small value float   tensor
module: autograd	backward of torch repeat slower than for torch repeat interleave
module: autograd	backward support for generalized eigenvalue solver with lobpcg in forward  only k rank symeig case 
module: autograd	torch autograd gradcheck support for tensor like types    torch function    
module: autograd	improvement of autograd engine original task placement in worker threads
module: autograd	unexpected gradient behavior for f interpolate or upsample
module: autograd	change how zero grad   behaves 
module: autograd	exception raised in backward   loses backtrace information
module: autograd	inplace operation error when using nn relu without setting inplace true
module: autograd	linear function and the different convs functions retain unnecessary buffers
module: autograd	internal assert failed for searchsorted with requires grad inputs 
module: autograd	follow the jax or tensorflow convention for meaning of grad   with complex inputs
module: autograd	wrong gradient for torch norm x  p float  inf    when input tensor has non unique max values
module: autograd	provide a convenient way for the user to reset the grad
module: autograd	generalize autograd test fn to add tests for complex dtypes
module: autograd	hooks registered to weight norm parameters cause runtimeerror
module: autograd	gradient anomaly after addition
module: autograd	searchsorted differentiable 
module: autograd	use undefined as tensor of zeroes in the python function api
module: autograd	libtorch c   setting gradient to   is incorrectly detected as violating const correctness
module: autograd	among torch autograd profiler profile and emit nvtx  whose overhead is smaller 
module: autograd	  dim    is not handled properly by the autograd codegen
module: autograd	add  inputs  argument to  autograd backward   
module: autograd	runtimeerror  only tensors of floating point dtype can require gradients
module: autograd	there is no interation between no grad  enable grad  set grad enabled
module: autograd	nn parameter overrides global gradient settings
module: autograd	create graph failed for custom cuda extension when calling torch autograd grad
module: autograd	throw error when embeddingbag      sparse true  and slice the embedding s weight
module: autograd	add undefined tensor gradient support to all backward functions
module: autograd	cuda memory leak when using backward with higher order gradients
module: autograd	modifying inplace views of leaf gives unclear error message
module: autograd	runtimeerror  one of the variables needed for gradient computation has been modified by an inplace operation 
module: autograd	nearest neighbor upsampling regression
module: autograd	module register backward hook doesn t work
module: autograd	range second   range first    t size   internal assert failed
module: autograd	runtimeerror  function  powbackward   returned nan values in its  st output 
module: autograd	gradcheck does not work for self overlapping inputs 
module: autograd	with emit nvtx  but got nothing except the word  torch autograd profiler emit nvtx object at  xxxxxxx 
module: autograd	parameter  incx     and shouldn t be  in cublas gemv in a gradient
module: autograd	warn the user when creating custom function with differentiable integer output 
module: autograd	print x grad  hang in autograd tutorial
module: autograd	we should not mark non floating point tensors as requirering gradients  ever
module: autograd	 discussion  expose future   in python api
module: autograd	hooks don t propagate error  try to call   name  
module: autograd	runtime error while calculating gradients   version       specific
module: autograd	in place leakyrelu backward calculation is triggered with a non positive slope which is not supported
module: autograd	fix  cdist  backward calculation for  p   
module: autograd	pytorch crashes while training a simple mnist classification problem
module: autograd	torch norm is numerically unstable at zero for multidim reductions
module: autograd	pytorch       requires grad being automatically set to false in c   registered operators
module: autograd	warnings from anomaly mode get eaten up by jupiter notebook  including on colab 
module: autograd	bad interaction between no grad and numpy conversion
module: autograd	added autograd support for c  c functions and enabled requires grad true for complex
module: autograd	test fmod cuda in test autograd is flaky
module: autograd	flakiness in testautograddevicetypecuda
module: autograd	explicitly define gradient for certain computation
module: autograd	 test cpp api autograd   customautogradtest deepreentrant is failing in tsan environment
module: autograd	fail to calculate gradients of gradients  
module: autograd	torch cdist gradients are nan for p   and very small differences in a given dimension    delta  e     
module: autograd	torch cuda amp gradscaler  runtimeerror
module: autograd	autograd grad not making higher order graph for scaling functions
module: autograd	runtimeerror  cowardly refusing to serialize non leaf tensor which requires grad  since autograd does not support crossing process boundaries
module: autograd	torch tensor new full ignores requires grad
module: autograd	native functions yaml  reset grad accumulator
module: autograd	incorrect view behavior for split split with sizes chunk
module: autograd	checkpointfunctionbackward not called if the output of the checkpointed function is an in place modified view
module: autograd	commit version          bce    c       b faf    c    e  cause dlrm benchmark performance drop
module: autograd	backward for tensor min   and tensor min dim    behaves differently
module: autograd	gradcheck for complex
module: autograd	gradient update of a sparse matrix results in a memory leak
module: autograd	pytorch     backward error
module: autograd	could you please add a torch setting like torch set checkpoint enabled true 
module: autograd	the output tensors are different when same code is implemented in jupyter and spyder  
module: autograd	legacy autograd function with non static forward method is deprecated and will be removed in     and userwarning  legacy autograd function object was called twice   you will probably get incorrect gradients from this computation  as the saved tensors from the second invocation will clobber the saved tensors from the first invocation   please consider rewriting your autograd function in the modern style  for information on the new format  please see  https   pytorch org docs stable notes extending html extending torch autograd    
module: autograd	allow accessing or save restore of savedvariables
module: autograd	end of the  data removal in torch optim
module: autograd	runtime error when using distributeddataparallel with torch no grad  
module: autograd	 autograd  fix allow unused checking for c   api
module: autograd	a minor bug in unused grad input check of run backward   
module: autograd	behavior of mixed derivatives is unexpected
module: autograd	some operations crash autograd if parameter size is changed 
module: autograd	scalar cuda tensors are automatically moved to different device
module: autograd	print the current node name in anomaly mode 
module: autograd	remove use of   data  from optimizers
module: autograd	remove  data from test test autograd py for fixathon
module: autograd	remove  data subset  for fixathon 
module: autograd	segmentation fault in autograd grad   during backward pass when using transformer layers
module: autograd	 nd derivative for im col and col im not implemented
module: autograd	add at  tensor  retain grad api
module: autograd	add a warning sign for anomaly detection
module: autograd	inconsistent handling of undefined tensors in gradient formulas
module: autograd	fix version counter bump in cpp function
module: autograd	runtimeerror  tensor does not have a device 
module: autograd	 feature request  custom sample size during backpropagation
module: autograd	issues with multithreading in autograd profiler
module: autograd	autograd fails if used before multiprocessing pool
module: autograd	failure on non contiguous gradients for f pad permute combination
module: autograd	 question pipeline batches inferencing
module: autograd	returning variables sharing storage with other variables that require grad is not supported in python functions  please submit a feature request if you hit this error 
module: autograd	loss backward    runtimeerror  sizes   called on undefined tensor
module: autograd	test cos scalar cpu in test autograd py  is flaky
module: autograd	second derivative fails if first derivative happens to be constant
module: autograd	segmentation fault  core dumped  with gradient updating 
module: autograd	torch autograd optimization is not working correctly when defined in torch autograd function static method
module: autograd	calling tensor backward   from within a c   extension hangs
module: autograd	 docs  missing docs for torch no grad  instead there are docs for torch autograd no grad
module: autograd	issue with gradient computations
module: autograd	 leaky relu   wrong gradient with negative slope
module: autograd	segmentation fault in  torch  autograd  engine  evaluate function   in      
module: autograd	improve gradient stability of logsumexp  softmax  log softmax  logsigmoid at  inf  replace nan by zero 
module: autograd	a strange issue about detection of tensor in place modification
module: autograd	 enable grad  context doesn t work as expected in backward function of torch autograd function
module: autograd	c   batchnorm   instancenorm modules register parameters incorrectly when affine is false
module: autograd	i cannot fix the error of variables has been modified by an inplace operation
module: autograd	cumsum backward for zero length zensors
module: autograd	torch no grad   context manager seems to leak memory
module: autograd	torch as tensor returns different value for  is leaf  for different dtypes 
module: autograd	fix silent autograd errors
module: autograd	no grad backward after  squeeze   or  unsqueeze   
module: autograd	assigning int values to autograd tensors by tensor data
module: autograd	add high level autograd functions
module: autograd	memory leak with layernorm and  nd derivatives  gradient penalty wgan 
module: autograd	running fp   softmax on a fp   tensor for dimensions other than the last fails during the backward pass
module: autograd	reshape with non contiguous input has wrong gradient on cuda  breaking einsum 
module: autograd	calculate grad error
module: autograd	gradients given by pytorch       and       are different when backward 
module: autograd	call to backward   obstructs script from terminating  maybe related to windows   
module: autograd	max norm in embedding layer modifies weight in place leading to issues with gradient backpropogation
module: autograd	weird interaction between sum  scalar multiplication and sparse tensor
module: autograd	error with detect anomaly and sparsetensor
module: autograd	extending pytorch  extending torch autograd have something wrong with output dimension setting to  
module: autograd	conversion to  bfloat     makes require grad false
module: autograd	 at  parallel for  does not propagate thread local variables to child threads in embedding renorm 
module: autograd	error then try to modify dropout value runtimeerror  one of the variables needed for gradient computation has been modified by an inplace operation
module: autograd	makes cuda tests in test autograd generic
module: autograd	make set grad accumulator private  friend class savedvariable 
module: autograd	make autogradmeta a private struct in variable 
module: autograd	add trailing underscore to member variable 
module: autograd	add trailing underscore to member variable 
module: autograd	notify workers of failure of distributed backward pass
module: autograd	clone   changed to clone at  memoryformat  contiguous 
module: autograd	at  nogradguard no grad
module: autograd	functioneventavg implements   iadd   interface
module: autograd	stops common utils py from setting the default tensor type  to torch doubletensor 
module: autograd	 rocm   rocm      enable roctx api
module: autograd	 docs  save for backward docs missing
module: autograd	 torch cdist  raises cuda error on backward with too big batch
module: autograd	gradient for d db a  b should be   at a   instead of nan
module: autograd	add memory format support  to  and  type  operators
module: autograd	 wip  testing  add memory format to to to to
module: autograd	add   torch function   api override mechanism
module: autograd	use grad out for cudnn ctc loss
module: autograd	distributed autograd   fast mode backward pass implementation 
module: autograd	 onnx  support exporting aten  copy  and aten  index put to onnx opset   
module: autograd	make typedefault  typederived and variabletype anonymous namespaces
module: autograd	 ignore  prop
module: autograd	unrelated element gets nan gradient
module: autograd	expands testautograddevicetype
module: autograd	enable double backward for non cudnn lstm and gru
module: autograd	add warning to anomaly mode doc fix       
module: autograd	in place after view in no grad  issue        
module: autograd	erfnet  pytorch  to onnx
module: autograd	c   api parity  at  tensor  register hook
module: autograd	c   api parity  at  tensor   version
module: autograd	fix tracing slice select with dynamic inputs
module: autograd	bug when an inplace op is done on the output of the forward of a autograd function and this output is a view of an intermediary result of the forward 
module: autograd	named tensor support for  atan   output nr  detach     requires grad 
module: autograd	f conv transpose d inside function doesn t use cudnn
module: autograd	gradient scaling api
module: autograd	add derivative for cholesky inverse
module: autograd	exception in autograd profiler total average   calculation
module: autograd	test naughty anomaly access xfail aborts test suite
module: autograd	delete tagged names
module: autograd	c   api parity  at  tensor  requires grad 
module: autograd	add noqengine to qengine
module: autograd	gradient for embedding vector at padding idx is not zero when on gpu
module: autograd	 rocm  enable basic gpu profiling capability on rocm 
module: autograd	fix cdist gradient computation if first arg is  xn
module: autograd	n a
module: autograd	adds generic device tests to test autograd py
module: autograd	implement more support for per channel quantization
module: autograd	c   api parity  at  tensor  version
module: autograd	c   api parity  at  tensor  output nr
module: autograd	c   api parity  at  tensor  is leaf
module: autograd	add derivative of cholesky solve
module: autograd	 bc breaking  c   api parity  at  tensor  grad
module: autograd	fix remaining invalid function cast warnings that show up with gcc    
module: autograd	 rocm  enable more mgpu tests
module: autograd	torch nonzero   leads to crash in pytorch    
module: autograd	expose a torch result type and simplify tensor iterator
module: autograd	c   api parity  at  tensor  data
module: autograd	skip testautograd test deep reentrant on macos
module: autograd	tensor expand does not respect torch no grad  
module: autograd	 onnx  avoid overwriting output type in onnx graph
module: autograd	python c   api parity  tensor autograd api
module: autograd	ignore f    in all   init   py without putting noqa
module: autograd	backward hook triggered despite removablehandle remove  
module: autograd	 pytorch  introduce intern disable autograd flag to create inference only library for mobile
module: autograd	implement initial version of autograd with named tensors
module: autograd	don t save  self  in  index  backward
module: autograd	delete a few cases where we directly use backend tensortypeid 
module: autograd	compare shapes of outputs and grad outputs in autograd grad
module: autograd	memory leak  autograd contexts not being garbage collected when assigned output tensors as attributes
module: autograd	delete totype const deprecatedtypeproperties       
module: autograd	fixed masking warnings in tests
module: autograd	no double backwards implemented for several rnn ops
module: autograd	creates torch friendly event class and adds stream tracking to autograd
module: autograd	removes symbolicvariable
module: autograd	make queue callback   work properly with reentrant backward
module: autograd	bcelosswithlogits input     bceloss sigmoid input  
module: autograd	add helper function tensor  names  
module: autograd	use a ptr to store autograd profiler rng
module: autograd	rename tensor  names   to tensor  opt names  
module: autograd	enable gradient scaling in autograd
module: autograd	use a ptr to store autograd profiler rng
module: autograd	attach  send  autograd function to the autograd graph as part of rpc 
module: autograd	run time error with backward    after upgrading to pytorch      
module: autograd	 wip  clang tidy fixes
module: autograd	in place modification detection does not work if a tensor does not own its storage
module: autograd	being able to read gradients from  register backward hook 
module: autograd	hooks for c   api
module: autograd	 autograd  c   api  bind autograd functions into c  
module: autograd	 do not merge  bisect windows good
module: autograd	 do not merge  bisect windows  
module: autograd	revert  move tensoroptions to aten core          
module: autograd	clang tidy   fix es
module: autograd	 v      retry  fixed bool in isintegraltype bug  plus review comments 
module: autograd	 retry  fixed bool in isintegraltype bug  plus review comments 
module: autograd	getting gradient of element of tensor wrt the element itself
module: autograd	fixed bool in isintegraltype bug  plus review comments 
module: autograd	fixed bool in isintegraltype bug
module: autograd	allow forward functions with single output to return variable
module: autograd	add warning for legacy autograd function apply  
module: autograd	        fix ctc loss for zero length targets on gpu         
module: autograd	back propagation trough slicing with list breaks
module: autograd	allow empty variables to be saved for backwards
module: autograd	 don t merge  don t close  v         stable docs job
module: autograd	conv d   conv d register backward hook inconsistency
module: autograd	support custom autograd functions in c  
module: autograd	maxpool d makes model not reproducible 
module: autograd	fix set grad for extension backends
module: autograd	 alband s         augmented with savedvariable  weak grad fn 
module: autograd	added torch autograd profiler record function   as context manager 
module: autograd	creates torch friendly event class   adds stream tracking to autograd
module: autograd	enabled cumsum and cumprod for bool tensors
module: autograd	rename tensor is named to has named  expose has named to python 
module: autograd	improve documentation for torch enable grad   torch no grad and torch set grad enabled
module: autograd	remove torch  autograd  node  get shared ptr  
module: autograd	fix ctc loss for zero length targets on gpu
module: autograd	rename torch  autograd  function to torch  autograd  node
module: autograd	api for autogradcontext
module: autograd	implement tensor set names   tensor names setter
module: autograd	fix for cdist backward for non batch tensors  do not review 
module: autograd	lint fix
module: autograd	fix lint
module: autograd	fix indexing for more than       elems in non indexed first dim
module: autograd	warn   clearly document that you can t use intermediate results in saved tensors for higher order derivatives
module: autograd	revert  improve performance of advanced indexing backward          
module: autograd	pass variable list of inputs to  wrap outputs
module: autograd	support custom autograd functions in c  
module: autograd	modify symmetric eigendecomposition derivative
module: autograd	avoid weak ptr  lock   when shared ptr is provably live  more error checking 
module: autograd	remove dead function  get shared ptr 
module: autograd	remove dead decref struct 
module: autograd	invert ownership between pyfunction and thpfunction 
module: autograd	remove usage of legacy autograd function
module: autograd	add warning for legacy autograd function
module: autograd	fix for cdist backward for non batch tensors
module: autograd	more documentation about the pyobj field 
module: autograd	 do not merge  test jenkins jobs after fixing depth  
module: autograd	zero gradients beyond a certain buffer size on cuda
module: autograd	make variable consumes the tensor if it only has one reference
module: autograd	handle gradients on dtypes changes without specifying explicitly
module: autograd	remove tensor data   call in python variable   and nn parameter   constructors
module: autograd	make pythonargs  tensor and pythonargs  scalar faster
module: autograd	remove rtti check for tensorimpl shadow copy
module: autograd	 bc breaking  remove legacy autograd function
module: autograd	make variable consumes the tensor if it only has one reference
module: autograd	refactoring  wrap outputs to remove python dependence 
module: autograd	 do not merge  caffe  requires grad check dev
module: autograd	skip test slogdet sign if lapack library is not installed
module: autograd	convert variableversion counter to intrusive ptr  saving a memory allocation on every tensor 
module: autograd	add gradmode  is enabled   check to requires grad  
module: autograd	pass variable into caffe  ops  by requiring that the variable doesn t require grad
module: autograd	add  requires grad  bool   api to tensor
module: autograd	add derivatives for the  weight  parameter of  torch lerp 
module: autograd	add optional  allow tensor metadata change  flag to detach  
module: autograd	pass variable into caffe  ops  by not checking   is variable    in caffe   tensor constructor
module: autograd	prioritize reentrant tasks and execute them recursively until close to limit
module: autograd	segmentation fault when returning from a function
module: autograd	expose is mkldnn to python and register it as torchscript prim op
module: autograd	thread local debug info
module: autograd	fix typos in gradcheck error message
module: autograd	auto differentiating torch cdist is broken on gpu
module: autograd	use lazy initialization in autograd record function to avoid static init ordering issues
module: autograd	 wip  v   pass variable into caffe  ops  by adding  at  autononvariabletypemode  to run   runasync  
module: autograd	improve handling of mixed type tensor operations
module: autograd	export deletefunction in torch csrc autograd function h
module: autograd	segmentation fault autograd 
module: autograd	move tensoroptions to aten core
module: autograd	replace type dispatch with atendispatch
module: autograd	checkpoint backward not called if function returns einsum result
module: autograd	move backward and set data off of type
module: autograd	cudnn ctc implementation ignores grad out completely
module: autograd	remove many usages of type
module: autograd	remove getdevicefromptr and allocator from type
module: autograd	move unsafetensorfromth and unsafestoragefromth off type
module: autograd	fix infinite loop in del post hook
module: autograd	fix infinite loop in del post hook
module: autograd	switch autograd to use a pool of workers for each device
module: autograd	cpp frontend torch  nll loss d memory leak
module: autograd	in place operation on differentiable view leaks memory
module: autograd	rename dimname  name to dimname  full name
module: autograd	document that no grad is thread local 
module: autograd	swap thread state before forwarding to python apply
module: autograd	fix with emit nvtx  also allow shape information to appear in nvtx ranges 
module: autograd	expose torch empty sizes     names       to python
module: autograd	with emit nvtx context manager is broken in current master 
module: autograd	 wip  swap thread state before forwarding to python apply
module: autograd	add  torch   future    overwrite module params on conversion  global flag  and check it in  nn module  apply   
module: autograd	numerical integration  trapz  function 
module: autograd	delete ddp hooks in reducer destructor
module: autograd	collapse tracing state h into tracer h
module: autograd	documentation for locking discipline in engine cpp  h
module: autograd	 jit  split out function into its own file
module: autograd	backward function will set a flag if it released variables
module: autograd	collapse tracing state h into tracer h
module: autograd	fix handling of kwargs from common method invocations
module: autograd	don t leak threads on exit
module: autograd	don t leak threads on exit
module: autograd	don t leak threads on exit
module: autograd	don t leak threads on exit
module: autograd	runtimeerror when trying to reuse a buffer
module: autograd	don t leak threads on exit
module: autograd	per callback sampling
module: autograd	 wip  move set requires grad bool  to variabletype
module: autograd	make ddp failure recoverable
module: autograd	documentation for locking discipline
module: autograd	 bc breaking  replace type dispatch with atendispatch
module: autograd	renaming member variables in engine cpp h
module: autograd	re add tensor t
module: autograd	after loss backwards   the model frozes
module: autograd	insert missing  pragma once in variabletypeutils h
module: autograd	merge interfaces that have an optional scalartype parameter
module: autograd	fix model to xla device 
module: autograd	added possibility to index scalars by bool masks
module: autograd	 assert no grad
module: autograd	allow nondet tol for nondeterminism in gradcheck and gradgradcheck
module: autograd	ctc loss  fix backward when  d target tensor is larger than max target length
module: autograd	remove thstensor and sparsetensorref
module: autograd	  wip  add channels last attribute to the tensor
module: autograd	 wip  test pr to showcase the comparison ops return type change
module: autograd	 do not merge  try to use  impl     in set data  
module: autograd	 do not merge  test different implementation in accumulategrad  apply
module: autograd	added some extra tests for std mean and var mean for multiple dims 
module: autograd	 pt   quant  add qscheme   method
module: autograd	finish removal of at check  officially deprecate the macro 
module: autograd	add  tensor t  attribute to reverse dimensions
module: autograd	add aten mkldnn batchnorm backward operator
module: autograd	 add aten mkldnn backward ops  max pool d  avg pool d and adaptive av 
module: autograd	add aten mkldnn backward ops  relu  linear and reshape
module: autograd	add aten mkldnn conv d backward operator
module: autograd	add  ndim  property to tensor
module: autograd	adding memory format to  empty and empty like operators
module: autograd	 wip  memory format part  
module: autograd	an outer torch no grad   is ignored inside a thread
module: autograd	require passing version counter and allow tensor metadata change to shallow copy and detach  
module: autograd	 wip  replace type dispatch with atendispatch
module: autograd	second order gradient cuda error
module: autograd	restore hidden visibility by default for linux builds
module: autograd	add autograd for to sparse 
module: autograd	memory format support for contiguous and is contiguous
module: autograd	replace at check with torch check  shard       
module: autograd	pytorch operator observer
module: autograd	 bc breaking  fix version counter sharing in set data  
module: autograd	backwards hangs
module: autograd	 rfc  make sparsetensorref the same as tensor
module: autograd	speed up recordfunction with sampled callbacks
module: autograd	numpy like nonzero  called nonzero tuple 
module: autograd	only record grad fn in c   scatter and gather when required so
module: autograd	 easy  profiler  improve repr for averaged events
module: autograd	prevent accidental casting to bool in ivalue
module: autograd	 wip  numpy arg translation proof of concept
module: autograd	pytorch profiler shape aggregation support
module: autograd	 draft  going to be two separate prs  add memory format support
module: autograd	torch  c  demangle  t   needs to return  t  instead of  unsigned short 
module: autograd	cleanup includes in torch csrc  
module: autograd	cleanup includes in torch csrc autograd  
module: autograd	cleanup includes in torch csrc autograd   
module: autograd	cleanup includes in torch csrc autograd functions basic ops h 
module: autograd	cleanup includes in torch csrc autograd saved variable h 
module: autograd	cleanup includes in torch csrc autograd python engine h 
module: autograd	cleanup includes in torch csrc autograd profiler h 
module: autograd	cleanup includes in torch csrc exceptions h 
module: autograd	remove redundant includes from torch csrc autograd variable h 
module: autograd	cleanup includes in torch csrc autograd functions basic ops h 
module: autograd	cleanup includes in torch csrc autograd saved variable h 
module: autograd	cleanup includes in torch csrc autograd python engine h 
module: autograd	cleanup includes in torch csrc autograd profiler h 
module: autograd	cleanup includes in torch csrc exceptions h 
module: autograd	remove redundant includes from torch csrc autograd variable h 
module: autograd	grad is none after using view
module: autograd	 wip  caffe  libtorch unification prep
module: autograd	 reland  enable assignment for qtensor in pytorch frontend
module: autograd	add is mkldnn to at  tensor
module: autograd	guard  set rebase  history on grad fn check
module: autograd	change some comments related to moving copy  to native
module: autograd	include c   namespace into torch
module: autograd	c   changes toward libtorch and libcaffe  unification
module: autograd	enable assignment for qtensor in pytorch frontend
module: autograd	document that autograd  profiler  recordfunction is available in python
module: autograd	option to enable disable autograd profiler  move chrome trace processing to c  
module: autograd	profiler  add self cpu time total  cpu time total and other general improvements
module: autograd	 with torch enable grad  also works outside a  no grad  context
module: autograd	can t call backward   in backward   
module: autograd	 hange type of a tensor with bools
module: autograd	switch autograd to use a flexible pool of workers with device locks
module: autograd	inconsistent recovery from cuda ooms
module: autograd	computing var stddev and mean at the same time
module: autograd	fail in repeated evaluation of  nd derivative of a custom autograd function
module: autograd	move gradmode   autogradmode   nogradguard to aten core
module: autograd	checkpointing evaluates irrelevant tasks
module: autograd	unserialize autograd
module: autograd	update argument names of torch  autograd  functionposthook
module: autograd	check that outputs and grad outputs have same shape in torch autograd grad   
module: autograd	cannot run multiple backward   in parallel  cpu execution is serialized
module: autograd	tensor unfold backward is slow
module: autograd	 bc breaking  remove variable  impl and differentiableviewimpl
module: autograd	use of  self  in legacy  non static  python autograd function leads to function which works on one gpu but not with dataparallel
module: autograd	missing gradient when autograd called inside a function on multi gpu  eg gradient penalty 
module: autograd	 grad input padding does not consider the dilation 
module: autograd	memory leak during backprop   in pytorch      
module: autograd	torch autograd grad bug after migrate from     to    
module: autograd	differentiable views don t work well with python autograd functions
module: autograd	inputs with output differentiability false don t bump version counter
module: autograd	assertion fails when using dataparallel with two nn embedding 
module: autograd	computing conv grads via nn grad raises valueerror when using dilation  
module: autograd	log and indexing do not commute correctly with respect to gradient
module: autograd	feedback about pytorch register backward hook
module: autograd	test autograd testautograd test ctc loss fails   cuda runtime error   too many resources requested
module: autograd	autograd engine clears callbacks at each invocation  breaking reentrant backwards callbacks
module: autograd	sparse scipy matrices are not accepted in autograd function parameters  but dense ones are 
module: autograd	in place operator      breaks autograd
module: autograd	autograd bug with inplace op
module: autograd	updates autograd engine to respect streams set in forward
module: autograd	backward parallelism causes performance degradation
module: autograd	use detach  to cut gradient propagation   but get a lower accuracy
module: autograd	recursionerror when using torch utils checkpoint
module: autograd	variable data returns a variable  but doesn t share version counters
module: autograd	determining  requires grad  automatically
module: autograd	segmentation fault when calling autograd grad   in autograd function backward
module: autograd	crash when autograd function returns list instead of tuple
module: autograd	concrete native function hazard
module: autograd	torch std nan gradient
module: autograd	rationalize functions signature specification in native functions yaml and derivatives yaml
module: autograd	feature request  support torch autograd grad   in optimizers
module: autograd	embeddingbag with data and offset intput misfunctions for  sample with empty sequence 
module: autograd	is grad attribute in variable immutable 
module: autograd	in place operation error thrown after mutation instead of before
module: autograd	more informative gradcheck return codes
module: autograd	raise a warning if someone tries to access  grad of a variable that will never be computed
module: autograd	inplace function tests in test autograd don t test inplace in the presence of other default args
module: autograd	 feature request  implementing operators for matrix backpropagation
module: autograd	check the type of grad given to backward and assigned to  grad
module: autograd	problem with backward hook function
module: autograd	implement remaining variable functions
module: tests	caffe   rfc increase the timeout for test lc  d
module: tests	fix test dispatch py when running with torch show cpp stacktraces  
module: tests	some test modules are using unittest main   instead of common utils run tests  
module: tests	 feature request  add a test option that allows tests continue to run after failure
module: tests	remove redundant tests from tensor op tests
module: tests	gradgradcheck for torch repeat and torch tile is outrageously slow
module: tests	when working on pr request         i encountered an error test variant consistency jit tensor split cpu bfloat  
module: tests	if test suite triggers cuda assert  should stop running tests
module: tests	test self assertequal with msg does not print numerical comparison results
module: tests	torch testing assert allclose doesn t check shapes
module: tests	ci no longer runs processgroup mpi tests
module: tests	add lazyconvxd and lazyconvtransposexd
module: tests	 big question why test nn use double dtype as cpu input and why set dtype prec dontuse 
module: tests	multi gpu distributed test is running on single gpu machine and fail
module: tests	testtensordeviceops auto generated tests are not actually being run
module: tests	fix some flaky tests in test torch py and test nn py
module: tests	remove event not ready assertion from testcuda test copy non blocking
module: tests	enable torch nn parallel comm typechecks during ci
module: tests	various tests wrongly decorated with largecudatensortest
module: tests	add batch updating parameter server example to ci tests
module: tests	v      rc    data loader test failures  unexpected keyword argument  persistent workers 
module: tests	test flakiness onoperator test locally connected op test py  testlocallyconnectedop  test lc  d
module: tests	fix windows test skipped in test test serialization py
module: tests	use smaller n to speed up testforeach
module: tests	enable torch nn modules   typechecks during ci
module: tests	support pytest for distribution testing
module: tests	wrong signature for torch max pool  d   d   d  in get testing overrides  
module: tests	fix cholesky tf   tests
module: tests	test torch py test inverse cuda causes illegal memory access on some platforms
module: tests	test nn py returns inconsistent result in different test setup
module: tests	test linalg py not run in ci
module: tests	test autograd test fails with rocm
module: tests	support opinfo based jit testing
module: tests	missing tests about torch xxx out     
module: tests	moves some of testtorchmathops to opinfos
module: tests	remove torch max min warning
module: tests	increase the memory requirement of test reduction split
module: tests	creates test tensor creation ops py test suite
module: tests	torch autograd gradcheck support for tensor like types    torch function    
module: tests	fix rocm ci by increasing test timeout
module: tests	adds linalg det alias  fixes outer alias  updates alias testing
module: tests	skips some complex tests on rocm
module: tests	fixes rocm ci
module: tests	skips spectral tests to prevent rocm build from timing out
module: tests	multiple test failures in processgroupncclerrorstest
module: tests	test on the correctness of type promotion between each two dtypes
module: tests	changed white allowlisted
module: tests	adds list of operator related information for testing
module: tests	test torch py fails if not build with mkl
module: tests	don t run tests with custom arguments with pytest
module: tests	running tests with   pytest fails with unrecognized arguments    subprocess
module: tests	 wip  adds test unary ufunc py
module: tests	test  add option to continue testing through error
module: tests	nameerror  name  get fail msg  is not defined  in test dataloader test proper exit
module: tests	clarifies compare with numpy behavior
module: tests	importing librosa is causing pytorch macos       py  test to fail
module: tests	testtorchdevicetypecpu test complex flip cpu complex      typeerror  list indices must be integers or slices  not tuple
module: tests	enh  refactor onnx expecttests
module: tests	test linspace takes too long to run
module: tests	rewrite documentation coverage tests
module: tests	non deterministic test failure in test test autograd py test profiler
module: tests	test fft input modification cpu should be skipped when no mkl
module: tests	unittest should print test immediately when it starts running
module: tests	pytorch bazel test doesn t report test failures in console
module: tests	added complex types to get all dtypes and turned on masked fill for complex
module: tests	testnn test cpu vs cudnn no dropout does not test miopen rnn on rocm
module: tests	turn on exact dtype by default on test optim py
module: tests	turn on exact dtype by default on test sparse py
module: tests	test jit py  is broken on py    
module: tests	adding and use  assertallclose  api in our tests  
module: tests	method overrides and instantiate device type tests don t play nicely
module: tests	some c  d tests are broken on macos
module: tests	re enable binary macos libtorch     cpu build and binary macos wheel     cpu build
module: tests	add a private api that lists overridable functions
module: tests	 rocm  fails on pdist tests
module: tests	build a generic failure injection mechanism for rpc
module: tests	magmainitializescorrectly cuda tries to take the inverse of a singular tensor
module: tests	make test test process group debug info in rpc test backend agnostic 
module: tests	test conv transposed large cuda failed on windows
module: tests	quantization unit tests are poorly named
module: tests	disabled test get gradients  dist autograd test distautogradjittestwithspawn 
module: tests	move hypothesis installation to docker
module: tests	test int pow cuda failed on windows
module: tests	testautograddevicetypecuda test to sparse cuda doesn t work
module: tests	test insert quant dequant multi uses is broken
module: tests	move pytorch distributed tests to separate folder for contbuild 
module: tests	allow setting tolerations in testing math functions 
module: tests	 run run test py has error
module: tests	 build  update googletest
module: tests	remove sharding code from tests
module: tests	the build short perf test gpu is broken on master
module: tests	changing the hypothesis dev verbosity to  normal 
module: tests	named tensor test big tensor repr requires default dtype to be set
module: tests	fail  test torch    main   testdoccoverage 
module: tests	removes  default floating dtype decorator
module: tests	don t run dist autograd fork on python  
module: tests	enable mgpu unit tests for rocm
module: tests	stops common utils py from setting the default tensor type  to torch doubletensor 
module: tests	move hipify to torch utils to bundle them into torch package
module: tests	python tests use torch doubletensor as their default tensor type
module: tests	 wip  v      quantization cherrypicks
module: tests	ci builds sometimes hang in or just after running test dataloader
module: tests	enabled bfloat   for cuda
module: tests	fix python rpc handler exit crash 
module: tests	rename  backend  to be  rpc backend  to be seperated from  communication backend  like gloo nccl  in  rpc test py 
module: tests	use  backend gloo  instead of   gloo   in rpc test py
module: tests	makes test cuda py s generated tensor op tests generic
module: tests	skip all rpc and dist autograd spawn tests for  py  
module: tests	rpc spawn test segfault on py  
module: tests	skip all rpc and dist autograd spawn tests for  py  
module: tests	enable distributed autograd tests for  py  
module: tests	 jit  skip tests that use numpy if it s not present
module: tests	 reland  fixed seek offset size to   bit 
module: tests	add test case for init rpc backend
module: tests	unify quantized conv and linear tests
module: tests	rpctest test nested rpc is flaky
module: tests	make rpc and dist autograd multiprocess test to use both fork and spawn
module: tests	 wip  clone empty supports memory format simplified
module: tests	hub improvements
module: tests	enable hub tests on macos
module: tests	bc check test was failing on friday and saturday
module: tests	makes test indexing py device generic
module: tests	in place after view in no grad  issue        
module: tests	 test do not merge  cuda      test  again
module: tests	disable qnnpack tests if pytorch is not built with it 
module: tests	enabled bfloat   dtype on cuda
module: tests	test naughty anomaly access xfail aborts test suite
module: tests	puts rocm tests on default stream
module: tests	removes torchtest  expands generic device testing
module: tests	serialization for per channel qtensor
module: tests	fix nuclear norm with requires grad true
module: tests	back out   pytorch  pr  refines test torch py generic device testing 
module: tests	refines test torch py generic device testing
module: tests	enabled bfloat   dtype on cuda
module: tests	 ez  ci  remove verbose in pytorch ci hypothesis profile
module: tests	run test cuda py under cuda memcheck in ci
module: tests	skip testhub on macos
module: tests	test utils testhub broken on macos 
module: tests	delay external imports until we re ready to test tensorboard
module: tests	fully deprecate variadic inputs of checkpoint sequential
module: tests	a few hub improvements
module: tests	skip testautograd test deep reentrant on macos
module: tests	 fix  indentation for hypothesis profile and proper inheritance for quantizationtestcase
module: tests	make running gloo tests conditional on availability
module: tests	refactor torch  solve tests
module: tests	make rpc and dist autograd multiprocess test to use both fork and spawn
module: tests	enable cpu fused kernel on windows
module: tests	 fix  derandomize hypothesis tests
module: tests	test c  d py  running with  pt option causes failing tests to pass
module: tests	add torch backends mkldnn enabled flag
module: tests	fix bug in assertnotequal for int tensors
module: tests	fixed masking warnings in tests
module: tests	distributionstest is failing on windows on master
module: tests	work around for bias quantization for conv and linear operators
module: tests	fix bug in assertnotequal for int tensors
module: tests	 jit  skip fstrings test if not py  
module: tests	test det logdet slogdet batched  in test cuda py  fails on ppc  le
module: tests	 do not review  channels last perf test
module: tests	ci warnings for quantization tests
module: tests	 jit  fix annotated assignment
module: tests	 jit  fix bugs in assignment to optionals
module: tests	disable tsan for test dataloader py 
module: tests	 jit  fix bugs in assignment to optionals
module: tests	test failures in test numba integration
module: tests	a lot of deprecation warnings in tests
module: tests	basic framework for distributed autograd context 
module: tests	let new and existing backends easily register for testing
module: tests	rebased str lib fix
module: tests	 jit  add support for multiple assignment statements
module: tests	 jit  move testrecursivescript
module: tests	don t require slow test reporting in  run tests py   pytest 
module: tests	 ci  add an option to bring certain tests to the front
module: tests	rename torchtest test all device types to torchtest for all device types
module: tests	enable broadcasting of batch dimensions rhs and lhs tensors for lu solve
module: tests	fix infer np scalar dtype mem leak
module: tests	 quantization  test nn quantized    test quantized nn mods
module: tests	remove completetensortype
module: tests	create tensor var mean variant
module: tests	 wip  scatter gather memory format
module: tests	fixed test image with boxes in test tensorboard py
module: tests	 tensorboard  added mesh tests
module: tests	 wip   add tensor iterator and some cuda functions memory propagation
module: tests	 torch distributed  rpc  add test case for nested rpc
module: tests	reverting https   github com pytorch pytorch pull         sync and async
module: tests	 rpc    worker deadlock if start  processgroupagent  join  with an unresolved future 
module: tests	 quantization  quantized linear  load from state dict and  save to state dict are untested
module: tests	 torch distributed  rpc   an rpc callee could crash on  rpcagent  join     if the caller terminates with unresolved future 
module: tests	 empty like    to    resize as   and  clone  now preserve memory format
module: tests	enable oss quantization tests         
module: tests	 jit  make torch jit attribute work with pytorch enabled  
module: tests	add python c   torch nn api parity test harness
module: tests	 jit  make torch jit attribute work with pytorch enabled  
module: tests	simplify tests that should cover all possible devices
module: tests	changed tensor comparison return type from uint  to bool         
module: tests	 quantization  enable oss quantization tests
module: tests	 wip  quantization  enable oss quantization tests
module: tests	 wip  quantization tests in oss
module: tests	 strings  fix split and rsplit implementations
module: tests	 wip  revert  changed tensor comparison return type from uint  to bool      
module: tests	 v       allowing batching for det logdet slogdet operations         
module: tests	 don t merge  don t close  v         stable docs job
module: tests	add scalar support for scatter add 
module: tests	python udf over rpc
module: tests	improvements on scatter and gather  prepare for implementing broadcasting
module: tests	allow scatter and gather to broadcast
module: tests	sync and async torch distributed rpc for builtin operators
module: tests	expose parse schema and   eq   function to python and add round trip tests
module: tests	support variadic returns in schema s operator  
module: tests	align the operator   for argument with functionschema parser
module: tests	align string str   with the format in functionschema
module: tests	minor refactor  propagating messages in testcase
module: tests	 wip  quantized average pool kernel
module: tests	 jit  make rhs  of assignment optional
module: tests	fix get all math dtypes for device  cuda  retuning none
module: tests	modify symmetric eigendecomposition derivative
module: tests	 tensorboard  cleanup api and remove  experimental  warning
module: tests	allowing batching for det logdet slogdet operations
module: tests	 wip  nestedtensor prototype
module: tests	 jit   script compile and  script class compile add to the python cu
module: tests	add common quantized test case utilities
module: tests	add mpi    initialization method for torch distributed
module: tests	rereapply optional scalartype interface changes that were reverted in d        
module: tests	properly formats errors rising up from c   extension compilation
module: tests	enable test torch py tests for bfloat   on cuda
module: tests	manual revert of d        
module: tests	fix two overindent lint errors in test common nn py 
module: tests	 jit  register   getitem   builtin
module: tests	improve handling of mixed type tensor operations
module: tests	re apply optional scalartype changes
module: tests	pin memory malloc now uses existing context if available 
module: tests	 jit   script compile and  script class compile add to the python cu
module: tests	disable test proper exit flaky worker kill
module: tests	remove thd
module: tests	reduce excessive ci printing in testhub
module: tests	enable promotion of tensor types in tensoriterator operations 
module: tests	use latest stable flake  bugbear in ci and fix b    flake  error 
module: tests	implementation of nn quantized linear module
module: tests	added bfloat   tensor for cpu with very limited support
module: tests	port symeig to aten and enable batching of inputs
module: tests	 jit  fix stdout capturing  remove some expect files
module: tests	 tensorboard  cleanup api and remove  experimental  warning
module: tests	test nuclear norm axes small brute force is flaky
module: tests	remove lowered execution
module: tests	some parts of pytorch test suite don t work properly on non default stream
module: tests	port svd to aten  enable batching for matrix inputs
module: tests	fix handling of kwargs from common method invocations
module: tests	 jit  move python   tests to their own file
module: tests	change pytorch tests to use non default cuda stream
module: tests	fix process group for tensors shared across processes
module: tests	don t leak threads on exit
module: tests	change pytorch tests to use non default cuda stream
module: tests	 jit  support for type annotations instead of torch jit annotate  
module: tests	 jit  add support for class annotations
module: tests	made floor ceil return ints
module: tests	 wip  tests
module: tests	add derivative for qr decomposition
module: tests	remove onnx export expects
module: tests	 pytorch  pr  functional conv d
module: tests	add handle to run all jit tests
module: tests	remove import torchvision when testing torch hub
module: tests	 bc breaking  changed tensor comparison return type from uint  to bool
module: tests	merge interfaces that have an optional scalartype parameter
module: tests	 testing   dataloader  delay cancel join thread until receiver exits
module: tests	 jit  improve error message on inferred type
module: tests	first class functions in ir  inlined eagerly
module: tests	deprecate variadic inputs of checkpoint sequential
module: tests	 jit  remove constant pooling expect
module: tests	test multinomial broken on rocm
module: tests	 tensorboard  add tests for add custom scalars and others
module: tests	test structseq repr intermittently segfaults
module: tests	 wip  test pr to showcase the comparison ops return type change
module: tests	add support for save and load mkldnn modules
module: tests	enable batched qr decomposition and add a  some  option
module: tests	added some extra tests for std mean and var mean for multiple dims 
module: tests	renaming the relu kernel and adding hypothesis tests
module: tests	 bc breaking  add scalar type info to tensor print
module: tests	quantized max pool op
module: tests	add autograd for to sparse 
module: tests	add matmul optimization for the case a ndim         b ndim     
module: tests	 onnx  improve onnx loop export
module: tests	assertequal in tests ignores dtype when comparing tensors 
module: tests	test namedtuple return is flakey
module: tests	 namedtensor  add test test namedtensor py
module: tests	serialize first class version of functions
module: tests	 jit  remove the addmm decomposation
module: tests	remove unnecessary printing from tests
module: tests	serialize first class version of functions
module: tests	serialize first class version of functions
module: tests	 onnx  add partial support for onnx tensor index export
module: tests	fix no sigchld checking in dataloaderiter  shutdown workers
module: tests	 wip  change comp ops result tensor type
module: tests	fix flaky store timeout test
module: tests	computing var stddev and mean at the same time
module: tests	test torch py  and other files  are too long  it takes a very long time to lint them
module: tests	give a good error message when forking after torch cuda device count  
module: tests	tests for vec    classes
module: tests	torch sigmoid behaves inconsistently for     and    bit nan inputs
module: tests	tensor multinomial dependent on shape for some reason
module: tests	 rfc  parameter pack class for testing
module: tests	don t use doubletensor as default global type in common py
module: tests	inplace function tests in test autograd don t test inplace in the presence of other default args
module: internals	 vmap  add batching rules for comparisons ops
module: internals	disable complex dispatch on min max functions
module: internals	change aten  native layer norm signature to match torch layer norm definition
module: internals	 torch tensor index put   mismatch between implementation and stub
module: internals	bad error message when int overflows
module: internals	c   s make optional is in the global namespace
module: internals	einsum ignore transpose  t  when input matrix dimension is big 
module: internals	add a test to detect dangling impl registrations with no corresponding defs
module: internals	move all the boolean flags within tensorimpl into a  bytes bitfield
module: internals	caffe  aten bridge doesn t handle overloads with ambiguous defaulting
module: internals	 onnx  exporting pytorch cumsum  v       to onnx  v       results in it being exported as aten dim      operator    cumsum        
module: internals	codegen should warn error when an argument that looks like an out argument isn t kwarg only
module: internals	torch as strided segfault when stride is empty tuple
module: internals	allow decentralized op registration 
module: internals	poor performance of dynamic quantazation
module: internals	scatter gather   check that inputs are of the same dimensionality
module: internals	libtorch tensor std    gets confused about which overload to use  expected vector  not scalar 
module: internals	 fr  add space as delimiter in torch check and other macros
module: internals	remove thcudamemgetinfo  use c   s cacheinfo instead 
module: internals	pytorch       requires grad being automatically set to false in c   registered operators
module: internals	 wip  reland addmv pr with blas disabled on mac os
module: internals	torch cdist produces nan gradients in pytorch      but not pytorch    
module: internals	remove the use of macro    hip arch    
module: internals	add mkldnn pooling backward
module: internals	cmake  add use system  libs cpuinfo sleef  options          
module: internals	standardize idioms for error context in c    error in c   util exception h
module: internals	put devicetype in torch namespace
module: internals	move the record stream to aten operation
module: internals	incorrect view behavior for split split with sizes chunk
module: internals	add  torch logcumsumexp 
module: internals	canonicalize includes in aten  and add tests for it
module: internals	use temporary variable to store input parameters in loop 
module: internals	deadlock on gcc     when running test nn py
module: internals	replace all uses of at index error with torch check index
module: internals	stop storing thplayout in the python argument parser
module: internals	tensor  copy should not copy when src and dst point to the same memory
module: internals	any reason to keep at warn 
module: internals	fix         uncaught domain error on macos 
module: internals	make storageimpl untyped for non pod types
module: internals	type conversion overflow causes uncaught std  domain error  mac only 
module: internals	bad error message when  out and non out variants don t match
module: internals	inconsistent behavior of zeros in certain operations with memory format channels last
module: internals	segfault when indexing with single element array
module: internals	fix windows build failure for no operator   in random iterator
module: internals	empty dispatch key set test in dispatchkeyextractor is unnecessary
module: internals	gaps for making template unboxing work for all operators
module: internals	std  string constructor for operatorname results in a lot of code goop at all call sites
module: internals	memory leaks and performance fixes
module: internals	intrusive ptr target should not have virtual methods
module: internals	keep tensor properties in jit   eager in sync
module: internals	tensor options creates schema mismatch for ops which have a  scalartype dtype  layout layout  device device  tuple
module: internals	requires grad argument is implicitly added to factory schemas
module: internals	binding variable methods functions in native functions yaml is awkward due to autononvariabletypemode
module: internals	expose a  dtype is signed 
module: internals	 c    deprecate tensor type    deprecatedtypeproperties 
module: internals	add intra op parallel support for the dynamic linear operator
module: internals	conversion to  bfloat     makes require grad false
module: internals	per tensortypeid fallthrough opt in
module: internals	 at  parallel for  does not propagate thread local variables to child threads in embedding renorm 
module: internals	make tensoriterator stop promoting types by copying
module: internals	move type casting to c   util typecast h
module: internals	bad value assignment using list indices when type promoting 
module: internals	torch device reporting an error as it should when using msnpu
module: internals	indexed  in place multiplication segfaults drops values
module: internals	 wip  move quantized  add scalar and quantized  add scalar relu to aten
module: internals	 jit  add type refinements for isinstance checks
module: internals	 rocm  add bfloat   support in linear algebra on rocm
module: internals	 don t close  docs ci        docs
module: internals	migrate soft margin loss from the th to aten  cuda cpu 
module: internals	devirtualize allow tensor metadata change   getter setter 
module: internals	cpu strided complex support for reduce ops and linpack ops
module: internals	add fused layer norm impl on cuda in pytorch
module: internals	move type inference for arange into c  
module: internals	add c   frontend   triplet margin loss
module: internals	fix the arithmetic overflow issue for msvc
module: internals	make jit serialization support arbitrary std  function   io
module: internals	add std  variant backport  mpark  as c    variant  with gcc       fix
module: internals	add new tensoraxes type  remove tensoroptions  operator    
module: internals	add memory format support to  zeros like  operator
module: internals	add memory format support to  rand like  operator
module: internals	clone   changed to clone at  memoryformat  contiguous 
module: internals	remove tensoroptions  operator    
module: internals	nonzero performance improvement
module: internals	avoid variable shadowing in     at  philox engine  single round    
module: internals	 include  stdexcept  into flat hash map h
module: internals	 include  stdexcept  into flat hash map h
module: internals	small fixes to improve tensoriterator overhead for the common case of inputs and outputs of the same type
module: internals	fix install location for aten core headers by avoiding relative paths
module: internals	 wip  change default ceil mode to true in avg pool d for back compatibility
module: internals	move hipify to torch utils to bundle them into torch package
module: internals	return notimplemented from all binary math ops
module: internals	wrapping namespace reduction in namespace at         
module: internals	use nnpack for strided convolutions 
module: internals	merge branch  master  of github com pytorch pytorch into driazati train 
module: internals	switching to profilinggraphexecutor  wip 
module: internals	move the cuda implementation of sqrt to aten 
module: internals	record stream   for shifted view tensors
module: internals	allow use cpu serial kernel with void lambda
module: internals	comprehensive ish instrumentation for cuda memory allocator
module: internals	use nnpack for strided convolutions 
module: internals	allow  align to  to take in partially named tensors
module: internals	 wip  v      quantization cherrypicks
module: internals	devirtualize numel  
module: internals	revert  make static dispatch turn off variable before entering the kernel           
module: internals	revert  add std  variant backport as c    variant          
module: internals	 wip cdist perf test
module: internals	allow use cpu serial kernel with void lambda
module: internals	add memory format support to  ones like  operator
module: internals	add memory format support to  full like  operator
module: internals	cleanup torch  jit  script  module api for accessing attributes parameters submodules 
module: internals	enabled bfloat   for cuda
module: internals	 quantization  move quantized  mul scalar to aten
module: internals	 quantization  move quantized  add scalar to aten
module: internals	 quantization  move quantized  mul scalar to aten
module: internals	add memory format support to  empty like  operator
module: internals	 quantization  move quantized  add scalar to aten
module: internals	 quantization  move tensor scalar add to aten
module: internals	add memory format support to typecasting shortcuts  byte   char   double   bool   half   int   long   short   float   bfloat   
module: internals	add memory format support to  cpu  and  cuda  operators
module: internals	make cpp backed jit classes appear as being in torch jit
module: internals	make static dispatch turn off variable before entering the kernel     
module: internals	 quantization  rename  intrinsic to intrinsic
module: internals	relax restrictions on set num threads
module: internals	refactor torch  jit  script  module  register   api 
module: internals	use functionschema instead of char  for dispatch
module: internals	work around a gcc   bug in building debug version of sleef
module: internals	use functionschema instead of char  for dispatch
module: internals	handle warning in torchscript
module: internals	proof of concept  remove tensor h dependence from caffe 
module: internals	serialize xla tensor
module: internals	serialize xla tensor
module: internals	remove reserve from push list elements on jit stack 
module: internals	 reland  fixed seek offset size to   bit 
module: internals	 pytorch  perf  use nnpack for strided convolutions 
module: internals	add memory format support  to  and  type  operators
module: internals	add memory format argument to the  clone  operator
module: internals	replace references to  dataloaderiter with  basedataloaderiter
module: internals	 wip  testing  add memory format to to to to
module: internals	make default string arguments  in schemas human readable
module: internals	remove tensor h  tensormethods h from src core 
module: internals	 jit  serializing autograd ops into its own namespace
module: internals	fixed seek offset size to   bit          for       
module: internals	add   torch function   api override mechanism
module: internals	implement pickle support for sparse tensors and torch layout instances
module: internals	fixed seek offset size to   bit 
module: internals	serialize xla tensor
module: internals	serialize xla tensor
module: internals	use grad out for cudnn ctc loss
module: internals	fix torch load for    gb file on windows
module: internals	migrate le gt ge eq ne from the th to aten  added support of type promotion 
module: internals	 v       move parallel for parallel reduce common implementation to cpp         
module: internals	do not attempt to add  o  for debug build for sleef 
module: internals	work around a gcc   bug in building debug version of sleef
module: internals	cleanup c    folder
module: internals	autograd  double backwards function for binary cross entropy loss
module: internals	migrate le gt ge eq ne from the th to aten  added support of type promotion 
module: internals	make nonzero non differentiable as it supposed to be
module: internals	better named tensor error messages 
module: internals	 pytorch  mobile  size  move parallel for parallel reduce common implementation to cpp
module: internals	choose num threads in parallel for based on grain size
module: internals	migrate ne from the th to aten
module: internals	migrate eq from the th to aten
module: internals	migrate le from the th to aten
module: internals	migrate ge from the th to aten
module: internals	migrate gt from the th to aten
module: internals	move the cuda implementation of log p to aten 
module: internals	migrate gt from the th to aten
module: internals	named tensor support for  index fill   index fill  squeeze  median tensor 
module: internals	make static dispatch turn off variable before entering the kernel 
module: internals	add memory format argument to the  clone  operator
module: internals	fix misuages for torch check torch internal assert with string
module: internals	fix issues in torch  tensor constructor
module: internals	choose num threads in parallel for based on grain size
module: internals	make typedefault  typederived and variabletype anonymous namespaces
module: internals	fix binary size in schema inference
module: internals	 onnx  export update for arange and  dim arange
module: internals	fix shared ptr binary size in op registration
module: internals	fix the bernoulli distribution sampler
module: internals	improve binary size of function schema inference
module: internals	change calling convention of atendispatch from getop to callunboxed 
module: internals	 ignore  prop
module: internals	fix parallelnativetbb build
module: internals	 wip  clone empty supports memory format simplified
module: internals	remove fbgemm is cpu supported in favor of torch backends quantized supported qengines
module: internals	add std  variant backport as c    variant
module: internals	add logcumsumexp function for tensor  wip 
module: internals	fix cuda named tensor  copy  
module: internals	fix c   registration binary size
module: internals	disable c   op registration temporarily to reduce code size
module: internals	switch static dispatch to use extractlegacytypeid 
module: internals	fixup
module: internals	make resize as  generic  so xla works 
module: internals	add some missing constructors to ivalue 
module: internals	add torch can cast from  to  function
module: internals	port l  loss to aten
module: internals	migrate  cosh  and  cosh   from th to aten  cuda 
module: internals	 reland  fix  quantized tensor tests
module: internals	move the cuda implementation of log  to aten 
module: internals	 jit  remove attemptorecovertype
module: internals	 jit  serialize autograd ops into its own namespace
module: internals	switch internal cuda build to c    
module: internals	add comments for multidim tensor factory limitations  and rename listinittensor for better clarity
module: internals	pytorch  try using rtld local instead of rtld global
module: internals	 pytorch  fix parallelnative h cpp build
module: internals	fix building with parallel backend native tbb
module: internals	 ivalue  fix future default constructor missing for parallelnative
module: internals	move the cuda implementation of log   to aten 
module: internals	tests for fallback boxed dispatch  including tls mode 
module: internals	add some missing constructors to ivalue 
module: internals	 pytorch  perf  add mobile friendly at parallel for backend
module: internals	don t generate named tensor functions to registrationfunctions h
module: internals	remove  dequantize per tensor
module: internals	 rename   per channel affine qtensor     make per channel quantized tensor
module: internals	 rename   per tensor affine qtensor     make per tensor quantized tensor
module: internals	per channel quantized tensor to have only a single axis
module: internals	 test do not merge  test for cuda    
module: internals	 ivalue  add tuple constructor   to std  tuple args     
module: internals	 jit  module dedupe
module: internals	 jit python none should have its type inferred as nonetype
module: internals	enable registering stackbased kernels with lambdas
module: internals	add torch promote types function
module: internals	import torch quantization when one imports torch
module: internals	port cuda sigmoid to aten cuda 
module: internals	add a lot of dimname overloads
module: internals	vectorize unary operator erfinv
module: internals	per channel fake quant
module: internals	update qengine flag in python to string
module: internals	caffe   typemeta uses compile time type names
module: internals	compile time type names
module: internals	c    string view
module: internals	add pybind version of handle th errors
module: internals	buffer python warning to avoid deadlocks
module: internals	make torch quantization part of import torch
module: internals	namespace reduction is not wrapped in torch namespace  c   api 
module: internals	aten port of lgamma  cuda 
module: internals	port cuda implementation of expm  to aten
module: internals	fix ci
module: internals	convert tensoriterator to use function ref  a lightweight alternative to std  function 
module: internals	address review comments in https   github com pytorch pytorch pull      
module: internals	 reland  unify quantization apis for add  pool and relu
module: internals	 rename   dequantize linear     dequantize per tensor
module: internals	 rename  quantize linear per channel    quantize per channel
module: internals	 rename  quantize linear    quantize per tensor
module: internals	 jit  allow  any   to appear as a type argument 
module: internals	c   api parity  at  tensor  register hook
module: internals	named tensor support for logsumexp  mode  kthvalue  median  min  max
module: internals	c   api parity  at  tensor   version
module: internals	wrap dimensions during named inference
module: internals	 test do not merge  cuda      test  again
module: internals	renames  tensor renamed    rename    tensor names     rename  
module: bc-breaking	cleaned up moduleattributeerror
module: bc-breaking	 pytorch  devirtualize tensorimpl  sizes   with macro
module: bc-breaking	 pytorch  devirtualize tensorimpl  dim   with macro
module: bc-breaking	 pytorch  devirtualize tensorimpl  numel   with macro
module: bc-breaking	preserve memory format in qconv op
module: bc-breaking	 fix  inplace remainder  
module: bc-breaking	using    performs an out of place operation instead of in place
module: bc-breaking	 numpy   torch angle   promote integer inputs to float
module: bc-breaking	making ops c   full  list of optional tensors
module: bc-breaking	stft  change require complex warning to an error
module: bc-breaking	enable distribution validation if   debug  
module: bc-breaking	fix remainder type promotion
module: bc-breaking	 numpy   torch digamma    promote integer inputs to float
module: bc-breaking	add missing complex support for torch norm and torch linalg norm
module: bc-breaking	fix  fmod  type promotion
module: bc-breaking	incorrect result of  fmod  and  remainder  operator 
module: bc-breaking	faithful out arguments
module: bc-breaking	a problem about elu backward
module: bc-breaking	fix type promotion for trace on cpu 
module: bc-breaking	fix type promotion   broadcasting bug
module: bc-breaking	randperm  add torch check to ensure generator device   tensor device
module: bc-breaking	pass optional by reference
module: bc-breaking	fix max pool d with ceil mode bug
module: bc-breaking	removed term blacklist in low net transform cc  glow net transform h  fakefp   transform cc 
module: bc-breaking	fixed einsum compatibility performance issues
module: bc-breaking	detect inplace modifications of views earlier  fix        
module: bc-breaking	fixed median nan propagation and implemented nanmedian
module: bc-breaking	preserve layout permutation for non dense non overlapping tensor in   like functions 
module: bc-breaking	fixed median nan propagation and implemented nanmedian
module: bc-breaking	use mta for amp grad unscaling  enforce op math type in mta functors  and allow op lambdas
module: bc-breaking	makes torch floor divide consistent with python and numpy and torch tensor   rtruediv   consistent with div s new true division behavior
module: bc-breaking	deprecates calling linspace and logspace without setting steps explicitly
module: bc-breaking	 resubmit  add amax amin
module: bc-breaking	add dilation to transposeconv s  output padding method
module: bc-breaking	properly check that reduction strings are valid for l  loss  smoothl  loss  and mse loss 
module: bc-breaking	evenly distribute output grad  into all matching inputs for min max median
module: bc-breaking	tensoriteratorconfig  check memory overlap by default
module: bc-breaking	standardized clamp kernels to numpy like implementation
module: bc-breaking	make torch conj   a composite function and return self for real tensors
module: bc-breaking	streamline stride propagation logic in tensoriterator
module: bc-breaking	temporary fix to batch norm running stats version bump
module: bc-breaking	 numpy compatibility   fix  argmin argmax  when multiple max min values
module: bc-breaking	updates torch tensor  torch as tensor  and sparse ctors to use the device of inputs tensors they re given  by default
module: bc-breaking	improve  torch norm  functionality  errors  and tests
module: bc-breaking	change c   frontend to take optional tensor  arguments
module: bc-breaking	support custom exception message
module: bc-breaking	feature request  torch isclose should set default atol and rtol based on the dtype of the tensors it s given
module: bc-breaking	reland split
module: bc-breaking	retain undefined tensors in backward pass
module: bc-breaking	change bceloss size mismatch warning into an error
module: bc-breaking	 pytorch  bump up variable version regardless of differentiability
module: bc-breaking	 onnx       update interpolate recompute scale factor default
module: bc-breaking	make torch norm consistent with numpy
module: bc-breaking	 wip  fix cpp grad accessor api
module: bc-breaking	use unbind for tensor   iter  
module: bc-breaking	fixed the moduledict key ordering problem
module: bc-breaking	throws runtime error when torch full would infer a float dtype from a bool or integral fill value
module: bc-breaking	 reland   change accumulategrad to yield   grad s that match weights  memory layout
module: bc-breaking	 jit  remove unnecessary clone apis for script  module and recursivescriptmodule
module: bc-breaking	libtorch tensor std    gets confused about which overload to use  expected vector  not scalar 
module: bc-breaking	adds dynamic versioning pattern
module: bc-breaking	overflow when binary shifting negative integer tensors stored on cpu
module: bc-breaking	split tensoriteratorconfig out of tensoriterator
module: bc-breaking	update schema to reflect aliasing behavior
module: bc-breaking	changes tensoriterator computation to not consider out kwarg  lets unaryops safe cast to out
module: bc-breaking	add undefined tensor gradient support to all backward functions
module: bc-breaking	unsafe split  unsafe split with sizes  unsafe chunk operations
module: bc-breaking	to fix extra memory allocation when using circular padding
module: bc-breaking	 verbose  unused in  torch backends cudnn 
module: bc-breaking	removes dunder div
module: bc-breaking	stops cross device data movement in tensor iterator
module: bc-breaking	change len dataloader  for iterabledataset
module: bc-breaking	throws runtime error when performing integer division using torch div
module: bc-breaking	migrate min dim    from thc to aten and remove  min
module: bc-breaking	fix the issue that pytorch doesn t construct bool tensors from non bo 
module: bc-breaking	update default value of recompute scale factor in interpolate
module: bc-breaking	remove datatype from storage and storageimpl
module: bc-breaking	change memory format promotion rules of point wise operators 
module: bc-breaking	 jit support adv indexing using list 
module: bc-breaking	stop creating integer type tensors that require gradients
module: bc-breaking	enforce tensor random  check that from and to are in tensor dtype bounds
module: bc-breaking	pytorch doesn t construct bool tensors from non bool values correctly
module: bc-breaking	suggestion for unify parameter name in nn conv and nn linear
module: bc-breaking	 torch pow  add type promotion support and fix issue with   rpow  
module: bc-breaking	add   torch function   for methods
module: bc-breaking	preserve references to buffers when doing module apply 
module: bc-breaking	 jit  fix torch tensor jit dtype
module: bc-breaking	make behavior of sobolengine consistent w  other rng functions
module: bc-breaking	torch quasirandom sobolengine      scramble true  does not respect torch manual seed
module: bc-breaking	     cherrypick   c   api parity   optimizers  merged optimizer and lossclosureoptimizer
module: bc-breaking	     cherrypick  bc breaking  fix c   api torch  nn parity bugs
module: bc-breaking	renaming  multilabelmarginlossfuncoptions    multilabelmarginlossfuncoptions  multilabelsoftmarginlossfuncoptions    multilabelsoftmarginlossfuncoptions
module: bc-breaking	fix f  interpolate and torch  nn  upsample implementation
module: bc-breaking	 c   api parity   optimizers  merged optimizer and lossclosureoptimizer
module: bc-breaking	change accumulategrad to yield   grad s that match weights  memory layout
module: bc-breaking	 c   api parity   optimizers  added closure to optimizers
module: bc-breaking	 c   api  update torch  nn functional docs
module: bc-breaking	 c   api parity  lbfgs optimizer step   update and added closure to the optimizer step   function
module: bc-breaking	makes floor divide a method  adds sparse floor division
module: bc-breaking	 c   api  remove deprecated torch  nn  batchnorm   featuredropout   modules ordered dict and torch  nn  init  nonlinearity   fanmode
module: bc-breaking	 c   api  rnn   gru   lstm layer refactoring
module: bc-breaking	fix         uncaught domain error on macos 
module: bc-breaking	end of the  data removal in torch optim
module: bc-breaking	port  remainder  from th to aten  cpu and cuda 
module: bc-breaking	remove deprecated codepath for old style autograd function         
module: bc-breaking	throw an error if nbytes is called on a sparse tensor 
module: bc-breaking	 c   api parity  adam  updated step and class design 
module: bc-breaking	remove use of   data  from optimizers
module: bc-breaking	 c   api parity  rmsprop optimizer update
module: bc-breaking	 bc breaking  rename at  tensor  base   to  base  
module: bc-breaking	 autograd  enable graph level thread parallelism on cpu
module: bc-breaking	sgd  updated step and class design
module: bc-breaking	move exponential  from th to aten  cpu 
module: bc-breaking	add option to use ninja to compile ahead of time cpp extensions
module: bc-breaking	bug fixes  torch  tensor floating point values     default dtype  and torch  tensor integer values    at  klong
module: bc-breaking	rename tensortypeid to dispatchkey
module: bc-breaking	port bceloss to aten to increase accuracy
module: bc-breaking	make conv       doptions and convtranspose       doptions different classes
module: bc-breaking	 bc breaking  multimargincriterion  fix scalar check in the case where reduction    none 
module: bc-breaking	 bc breaking  change index select scalar check to retain dimensionality of input 
module: bc-breaking	 bc breaking  fix scalar check of multilabelmarginloss 
module: bc-breaking	 bc breaking  change index select scalar check to retain dimensionality of input 
module: bc-breaking	consistently correctly handle numpy integral types
module: bc-breaking	remove dead python code that uses  data
module: bc-breaking	make all optimizers consistent so that they don t change gradients inplace
module: bc-breaking	switch default memory format of clone operator to preserve
module: bc-breaking	 bc breaking  turn off scalar check for masked select 
module: bc-breaking	use default dtype for torch  tensor floating point values  and torch  tensor empty braced init list  when dtype is not specified
module: bc-breaking	adagrad optimizer   updated step function  added param groups  state to optimizers
module: bc-breaking	 c    deprecate tensor type    deprecatedtypeproperties 
module: bc-breaking	use pybind    gil scoped   functions instead of autogil autonogil
module: bc-breaking	use at  klong for torch  tensor integer value  when dtype is not specified
module: bc-breaking	c   python api parity for conv       d layers  and add f  conv       d functionals
module: bc-breaking	merge tensor and variable 
module: bc-breaking	 bc breaking  fix bugs in torch  tensor constructor
module: bc-breaking	fix scalar handling of unfold 
module: bc-breaking	the docstring for torch addcdiv can t be correct
module: bc-breaking	return   numel empty tensor from symeig when eigenvectors false
module: bc-breaking	use c    variant based enums for embeddingbag mode
module: bc-breaking	onnx interpolate add scales params
module: bc-breaking	merge tensor and variable types 
module: bc-breaking	 bc breaking  fix bugs in  torch  tensor  constructor
module: bc-breaking	preserve original tensoriterator behavior when not explicitly promoting
module: bc-breaking	use c    variant based enums for reduction
module: bc-breaking	torch tensor numpy float      creates a float   tensor
module: bc-breaking	torch histc added a finite range check to resolve segfaults if tensor has inf  also added checks for nan values  min max
module: bc-breaking	move type inference for arange into c  
module: bc-breaking	torch histc segfaults if array has inf
module: bc-breaking	 bc breaking  c   api parity  linear
module: bc-breaking	torch full is inconsistent with np full
module: bc-breaking	remove deprecated torch gels
module: bc-breaking	change schedulers to chainable form
module: bc-breaking	 bc breaking  make  options name   private  and change all callsites to use  options name   
module: bc-breaking	implement torch nn embedding   embeddingbag in pytorch c   api
module: bc-breaking	 bc breaking  c   api parity  at  tensor  grad
module: bc-breaking	 bc breaking  torch flatten   returns a   dim tensor on a   dim tensor
module: bc-breaking	 bc breaking  fix aliasanalysiskind  pure on msvc
module: bc-breaking	switch hub to use  requests  because of ssl
module: bc-breaking	 bc breaking  add align corners option to grid sample and affine grid  change default to false
module: bc-breaking	 fr  more consistent matrix norm for torch norm
module: bc-breaking	change schedulers to chainable form
module: bc-breaking	port addcdiv operator from the th code to aten
module: bc-breaking	tensoriterator  binary op input output overlap check
module: bc-breaking	 data loader  make more iterator attributes private
module: bc-breaking	adam implementation minor fix
module: bc-breaking	port addcdiv operator from the th code to aten
module: bc-breaking	port atan  from th to aten
module: bc-breaking	port  pow  operator from the th code to aten
module: bc-breaking	torch flatten   returns a   dim  not   dim  tensor for   dim tensors
module: bc-breaking	ensure invert operator change gets into    
module: bc-breaking	remove deprecated linear algebra functions  and methods 
module: bc-breaking	remove deprecated linear algebra wrappers
module: bc-breaking	remove tensor data   call in python variable   and nn parameter   constructors
module: bc-breaking	 bc breaking  remove legacy autograd function
module: bc-breaking	adam adamw implementation minor fix
module: bc-breaking	delegate python    invert operator  to tensor bitwise not   
module: bc-breaking	improve handling of mixed type tensor operations
module: bc-breaking	inconsistent ctc loss api
module: bc-breaking	return device object instead of device index for torch cuda current device
module: bc-breaking	enable promotion of tensor types in tensoriterator operations 
module: bc-breaking	 bc breaking  use fn param  instead of fn param data  in nn module  apply
module: bc-breaking	made floor ceil return ints
module: bc-breaking	 bc breaking  changed tensor comparison return type from uint  to bool
module: bc-breaking	 jit  made floor ceil return ints  yay python    
module: bc-breaking	merge interfaces that have an optional scalartype parameter
module: bc-breaking	 second try   bc breaking  shallow copy indices and values in sparse tensor ctor
module: bc-breaking	 bc breaking  add scalar type info to tensor print
module: bc-breaking	 bc breaking  fix version counter sharing in set data  
module: bc-breaking	 bc breaking  shallow copy indices and values in sparse tensor ctor
module: bc-breaking	 hange type of a tensor with bools
module: bc-breaking	strip doc string from exported onnx models
module: bc-breaking	 fr  make torch nn utils convert sync batchnorm a classmethod of syncbatchnorm
module: bc-breaking	kill backend constructor of tensoroptions 
module: bc-breaking	introduce deprecatedtypeproperties class
module: bc-breaking	 bc breaking  remove variable  impl and differentiableviewimpl
module: bc-breaking	use of  self  in legacy  non static  python autograd function leads to function which works on one gpu but not with dataparallel
module: bc-breaking	return namedtuples from torch   function with multiple return arguments for c   operators
module: bc-breaking	torch full and torch randint are inconsistent in arg order
module: bc-breaking	 bc breaking  fix batch norm multiplier init
module: bc-breaking	nn module hook fix and improvements
module: bc-breaking	expunge torch utils trainer  
module: bc-breaking	fix bug in grad py when conv bias    none
module: bc-breaking	remove caffe   tensor  capacity nbytes  at  tensor  to  name  data 
module: bc-breaking	remove implicit conversion from gpu to cpu
module: bc-breaking	tensor chunk returns wrong number of chunks
module: bc-breaking	fix lr scheduler s last epoch value at the time of initialization  bc breaking  
module: bc-breaking	linearly interpolating upsampling fix
module: bc-breaking	    issues tracking
module: bc-breaking	bilinear upsampling fix
module: bc-breaking	variable data returns a variable  but doesn t share version counters
module: bc-breaking	better default inits
module: bc-breaking	torch std nan gradient
module: build	tools  move sha check to else statement
module: build	don t run git when pytorch build version is set
module: build	when i was compiling  these errors appeared  what should i do
module: build	new vsx support breaks compilation if using g   v   ppc  le 
module: build	build broken on ubuntu due to mkl symbols not found
module: build	in pytorch version        libtorch python so is incompatible with libshm so  why 
module: build	subprocess calledprocesserror  command    cmake      build           target    install      config    release           j          returned non zero exit status   
module: build	subprocess calledprocesserror  command    cmake      build           target    install      config    release           j         returned non zero exit status   
module: build	possible bug  not able to build on mac os
module: build	fix warning when running scripts build ios sh
module: build	undefined reference to   ros         with ros 
module: build	packagesnotfounderror  the following packages are not available from current channels 
module: build	can t build pytorch to support metal
module: build	refactor  don t specify cuda complete version manually in windows build scripts 
module: build	issue         only set the cmake imported location property in static 
module: build	checking the hw optional arm   sysctl prevents apple silicon from building a universal binary
module: build	aarch   cmake checks are incorrect on apple silicon
module: build	problem with building from source in win 
module: build	error  a member of type  const c    symbol  cannot have an in class initializer on windows
module: build	torchconfig cmake setting imported location unconditionally on torch import target
module: build	error for c   cuda extension build 
module: build	torch  stop using  nt quote args from distutils
module: build	cpp extension building with windows fails for python    
module: build	error in fully qualified type name impl   on windows    funcsig   doesn t match
module: build	sleef build failure when building pytorch incrementally 
module: build	compilation issue with visual studio v       and higher
module: build	v      block while model cuda  
module: build	figure out a better way to find libuv on windows
module: build	replace kernel resource strings with real  cu source files
module: build	 v       various setup py fixes
module: build	enable pytorch compilation on apple silicon
module: build	inconsistent environment variable naming for setting nvtoolext home in torchconfig cmake
module: build	linker error when building from source with cuda     
module: build	runtimeerror  nvrtc  error  invalid value for   gpu architecture   arch 
module: build	attributeerror  module  typing  has no attribute   classvar 
module: build	nccl build errors  unable to checkout  cd     in submodule path  third party nccl nccl  and unable to build nccl external
module: build	update how to build pytorch with cuda windows instructions
module: build	pytorch doesn t build with cuda     
module: build	compilation of binaryopskernel cpp using gcc       fails with internal compiler error after       
module: build	cuda      compilation flags incorrect for extensions
module: build	attributeerror  module  torch  c  has no attribute   pybind   compiler type 
module: build	xnnpack src qu  requantization precise psimd c        error  unrecognizable insn 
module: build	question about build torch whl package
module: build	error building c   cuda extension with pytorch cuda c   in msvc using cmake
module: build	 torch version debug  returns true for release builds
module: build	torch utils collect env return true in is debug build
module: build	conditional requirement for py    only
module: build	v    requirement  dataclasses  is not compatible with python      
module: build	adding link to gcov depending on gcc version
module: build	error with installing through pip
module: build	could not load library cudnn ops infer     dll  error code    
module: build	the different train speed between pip install torch      and build torch    from source
module: build	help please  segment fault when destruction after dlopen torch lib
module: build	bazel build failed
module: build	cuda problem with linux kernel    
module: build	cuda files in windows binary builds are not triggering sccache
module: build	windows binary builds are failing  c  program  is not recognized as an internal or external command
module: build	binary builds for wheels are failing  undefined reference to  mkl lapack claed  
module: build	setup  dataclasses only when      
module: build	c     internal compiler error when building pytorch on windows
module: build	bump nightlies to      
module: build	setup  only include dataclasses for py      
module: build	statically linking to libtorch but also needing the shared library  libcaffe  nvrtc so  
module: build	 build   cmake   rocm  find hsa runtime   properly
module: build	compile error for  cu files when using libtorch
module: build	add dllexport before template specialization functions for windows build
module: build	hipify revamp
module: build	blank value for  glibcxx use cxx   abi when compiling with homebrew s gcc  on macos 
module: build	install aten native cuda and hip headers
module: build	cxxabi   and glibcxx   not found on gcc       after building pre cxx   abi libtorch from source using gcc      
module: build	windows source build fails with  error lnk      at linking stage
module: build	static build
module: build	static build error
module: build	attributeerror  module  torch  c  has no attribute  extrafilesmap 
module: build	libtorch load trace module failed in qt 
module: build	build failed pytorch     on ubuntu      
module: build	something goes wrong with pytorch build from source 
module: build	 windows cuda build  unknown option   xcompiler  w  w 
module: build	hang when compiling static libs on windows
module: build	how to package pytorch with the file build from source 
module: build	adding  build profile to  gitignore
module: build	pytorch rocm        cmake cannot find hsa runtime  
module: build	remove pybind   from required submodules
module: build	error   vst q f   x   was not declared in this scope
module: build	disable instruction set when build libtorch
module: build	unable to import pytorch
module: build	segfault together with  import cv  
module: build	nightly py error
module: build	ubuntu      build from source error
module: build	build error when building on android 
module: build	ubuntu      install pytorch support gpu from sources
module: build	torch include aten core ivalue inl h    cl exe  failed with exit status  
module: build	introduce build caffe  flag
module: build	windows nightly build failed  cuda   debug   libtorch 
module: build	added rocm     docker image
module: build	workspace c has mkldnn   false  caffe  mkldnn using possible 
module: build	move wholearchive to link option
module: build	from sources install pytorch on ubuntu      the question is this what should i do  
module: build	move caffe  onnx main py    clang  ubuntu      off of caffe  image
module: build	neon intrinsic types issue
module: build	make windows cuda    tests master only
module: build	link error in windows build
module: build	bazel build has warnings
module: build	build from source fail
module: build	 cmake  end support for python    for pytorch
module: build	build test e e tensorpipe only if gloo is enabled
module: build	pytorch binaries for macos has serial at  parallel for because it is built with openmp which  is not available
module: build	pytorch     build fails on red hat
module: build	importing pytorch won t load cuda symbols unless forced by torch use rtld global
module: build	failed to build with cuda    on windows when system protobuf is used   ambiguous operator overload 
module: build	enable distributed package on windows  gloo backend supported only
module: build	don t proceed into setup py too far if python version is unsupported
module: build	cant install pytorch
module: build	git clone fails with  unable to checkout  f   d      c a  be  b ebb  b b bb   b     in submodule path  third party tensorpipe  
module: build	build failed on raspberry pi  fatal error  gloo algorithm h  no such file or directory
module: build	linking torch cpu files if compiled in a path containing whitespace
module: build	caffe  error  more than one operator     matches these operands  windows and cuda   
module: build	is there another way to install pytorch in the develop mode 
module: build	nccl alltoall process group introducing time out of other nccl tests
module: build	fatal error  tensorpipe tensorpipe h  no such file or directory
module: build	 rocm   build error   caffe    cuda h cuda runtime h  file not found
module: build	missing torch after some home modifications
module: build	undefined reference to fbgemm 
module: build	aten looking for function cusparsedcsrmm  in cuda    header  windows
module: build	add cusolver to build  rewrite magma inverse with cusolver
module: build	 macos   nightlies  dlopen     library not loaded   rpath libuv   dylib j
module: build	pytorch        linking error when linking  libtorch cuda dylib  on macos         with cuda support
module: build	compilation error  performing test support glibcxx use c   failed 
module: build	error  compiled pytorch from source on power  machine cannot import name   add docstr  from  torch  c   unknown location    
module: build	libtorch  torch  cuda  is available   returns zero
module: build	don t add nccl dependency to gloo if system nccl is used
module: build	system pybind   is not considered when using setup py
module: build	 rocm   build error   cmake  cannot find hip
module: build	build android sh build fail
module: build	system hangs while building from source
module: build	pytorch was compiled without numpy support
module: build	cudnn  version check fails
module: build	c   build fails on some machines
module: build	 home lc pytorch cxx libtorch include torch csrc api include torch nn cloneable h        error  no matching function for call to  at  tensor  to const c    device   const 
module: build	test cpp extensions aot   fail on power
module: build	 ci  docker image not found
module: build	submodules redownloaded even when present for the case of  q nnpack not available
module: build	linux build failing
module: build	when i import pytorch   there is a importerror library not loaded   rpath libmkl core dylib
module: build	linux libtorch c   cmake not compatible with newer versions of cudnn  for caffe  
module: build	build error  use of enum  cudagraphexecupdateresult  without previous declaration
module: build	work on pytorch python code without having to build from source
module: build	unable to build cuda extension with pytorch    
module: build	aten looking for function cusparsedcsrmm  in cuda        header but this is no more
module: build	cusparse library not found in cuda installation
module: build	libtorch       wsizeof array div
module: build	combining libtorch c   api with  pure  cuda code
module: build	 libtorch  build failed with opencv       using cmake 
module: build	c api pollutes the global namespace
module: build	error  torchvision has requirement torch which is incompatible
module: build	build pytorch android from source failed with msb     error during cmake
module: build	cannot build pytorch       from source
module: build	fix nccl library
module: build	weak symbols are resolved to the avx version even on non avx machine
module: build	build libtorch cxx   abi shared with deps       cu   zip   creat a tensor in cpu is ok  but transfer to gpu error
module: build	building from source with conda gcc
module: build	integrationtest cartpole is flaky
module: build	libtorch       build from source failed on cent os  
module: build	gcc     compilation error on master
module: build	cuda debug build failed on windows
module: build	invalid conversion from  caffe   blob   to  uint   t  aka unsigned int  
module: build	delay loading the cuda library on windows
module: build	better testing on cpus without avx capabilities
module: build	using loadlibraryex and load library search   flag for loading dlls o 
module: build	pytorch     conda package disallows openblas 
module: build	defaulting to ninja build doesn t forward includes in pytorch c   cuda extensions
module: build	lack of avx  not detected correctly in build
module: build	add a ci job using conda compilers
module: build	fix readme for installation from source
module: build	missing tools nnwrap module on completely clean win    install
module: flaky-tests	disabled test device maps multi gpu self    main   tensorpipetensorpipeagentrpctestwithspawn 
module: flaky-tests	disabled test device map gpu mixed self      main   tensorpipetensorpipeagentrpctestwithspawn 
module: flaky-tests	pytorch linux bionic py    gcc  coverage test  intermittently fails due to the no output timeout
module: flaky-tests	 no data to combine  error intermittently on windows cuda test builds
module: flaky-tests	disabled test all reduce sum cuda async    main   testdistbackendwithfork 
module: flaky-tests	disabled test reduce multigpu    main   testdistbackendwithfork 
module: flaky-tests	disabled test reduce sum cuda twice    main   testdistbackendwithfork 
module: flaky-tests	test async function remote multi    main   processgroupjitrpctestwithspawn  is flaky
module: flaky-tests	fix some flaky tests in test torch py and test nn py
module: flaky-tests	remove event not ready assertion from testcuda test copy non blocking
module: flaky-tests	test flakiness onoperator test locally connected op test py  testlocallyconnectedop  test lc  d
module: flaky-tests	disabled test dist optim functional    main   processgroupdistoptimizertestwithspawn 
module: flaky-tests	disabled test backward ddp outside    main   tensorpipeddpunderdistautogradtestwithspawn 
module: flaky-tests	disabled test backward ddp inside    main   tensorpipeddpunderdistautogradtestwithspawn 
module: flaky-tests	disabled test backward ddp outside uneven inputs    main   tensorpipeddpunderdistautogradtestwithspawn 
module: flaky-tests	disabled test backward ddp outside    main   tensorpipeddpunderdistautogradtestwithspawn 
module: flaky-tests	disabled test backward ddp inside    main   tensorpipeddpunderdistautogradtestwithspawn 
module: flaky-tests	disabled test backward ddp outside uneven inputs    main   processgroupddpunderdistautogradtestwithspawn 
module: flaky-tests	test rpc spawn fails sporadically
module: flaky-tests	disabled test udf remote message delay timeout to self    main   faultyagentrpctestwithspawn 
module: flaky-tests	disabled test ddp dist autograd local vs remote    main   testddpcomparisontensorpipe 
module: flaky-tests	disabled test ddp dist autograd local vs remote    main   testddpcomparison 
module: flaky-tests	disabled test backward ddp inside    main   processgroupddpunderdistautogradtestwithspawn 
module: flaky-tests	disabled test ddp dist autograd local vs remote    main   testddpcomparison 
module: flaky-tests	disabled test backward ddp outside    main   processgroupddpunderdistautogradtestwithspawn 
module: flaky-tests	disabled test backward no ddp    main   testddpunderdistautograd 
module: flaky-tests	disabled test forward async    main   remotemoduletestwithspawn 
module: flaky-tests	flaky test test multiple backward with errors    main   tensorpipeagentdistautogradtestwithspawn 
module: flaky-tests	 ci  fail  test n threads    main   testopenmp parallelfor 
module: flaky-tests	disabled test handle send exceptions    main   tensorpipeagentrpctestwithspawn 
module: flaky-tests	disabled test backward node failure python udf    main   tensorpipeagentdistautogradtestwithspawn 
module: flaky-tests	disabled test backward node failure    main   tensorpipeagentdistautogradtestwithspawn 
module: flaky-tests	disabled test graph for python remote call    main   distautogradtestwithspawn 
module: flaky-tests	integrationtest cartpole is flaky
module: flaky-tests	disabled test reduce scatter    main   testnccl 
module: flaky-tests	disabled test reduce    main   testnccl 
module: flaky-tests	disabled test broadcast    main   testnccl 
module: flaky-tests	disabled test memory profiler    main   testautograd 
module: flaky-tests	disabled test trainer ps    main   tensorpipeagentdistautogradtestwithspawn 
module: flaky-tests	disabled test call method on rref    main   tensorpipeagentrpctestwithspawn 
module: flaky-tests	disabled test rref proxy reuse    main   tensorpipeagentrpctestwithspawn 
module: flaky-tests	disabled test backward node failure    main   tensorpipeagentdistautogradtestwithspawn 
module: flaky-tests	disabled test remote script module    main   jitrpctestwithspawn 
module: flaky-tests	distautogradtest testinitializedcontextcleanup and distautogradtest testinitializedcontextcleanupsendfunction fail
module: flaky-tests	disabled test embedding bag with no grad tensors    main   distautogradtestwithspawn 
module: flaky-tests	disabled test backward multiple output tensors    main   distautogradtestwithspawn 
module: flaky-tests	 ci  test broadcast coalesced    main   testcuda   memory exception on virtual address
module: flaky-tests	disabled test self remote rref as rpc arg    main   rpctestwithspawn 
module: flaky-tests	disabled test graph for builtin call    main   distautogradtestwithspawn 
module: flaky-tests	disabled test backward node failure    main   distautogradtestwithspawn 
module: flaky-tests	disabled test barrier group cuda    main   testdistbackend 
module: flaky-tests	disabled test barrier full group cuda    main   testdistbackend 
module: flaky-tests	disabled test profiler with sync rpc udf    main   rpctestwithspawn 
module: flaky-tests	disabled test profiler with async rpc udf    main   rpctestwithspawn 
module: flaky-tests	disabled test multi rpc    main   rpctestwithspawn 
module: flaky-tests	disabled test clean context during backward    main   distautogradtestwithspawn 
module: flaky-tests	 ci  test distributed   test all gather multigpu    main   testdistbackend 
module: flaky-tests	disabled test profiler with async rpc builtin    main   rpctestwithspawn 
module: flaky-tests	test fmod cuda in test autograd is flaky
module: flaky-tests	flakiness in testautograddevicetypecuda
module: flaky-tests	pytorch linux xenial py  clang  android ndk r  c docker build job is flaky
module: flaky-tests	intermittent hang in buildlivenesssets after test svd cuda float   on cuda      configurations
module: flaky-tests	disabled test check failed messages    main   faultyagentrpctestwithspawn 
module: flaky-tests	test avg pool d nhwc is failing on parallel builds  like pytorch parallelnative linux xenial py    gcc    test 
module: flaky-tests	test grad copy sparse indices extra ref is flaky
module: flaky-tests	 testautograd test leaky relu inplace with neg slope  fails intermittently on rocm
module: flaky-tests	deadlock on gcc     when running test nn py
module: flaky-tests	test backward node failure python udf is flaky
module: flaky-tests	disabled test autograd context    main   distautogradtestwithspawn 
module: flaky-tests	disabled test backward node failure    main   distautogradtestwithspawn 
module: flaky-tests	installing cuda intermittently fails on windows
module: flaky-tests	network based torchhub tests contribute to ci flakiness
module: flaky-tests	disabled test stress heavy rpc torchscript    main   rpctestwithspawn 
module: flaky-tests	disabled test torchscript function exception    main   jitrpctestwithspawn 
module: flaky-tests	disabled test torchscript function    main   jitrpctestwithspawn 
module: flaky-tests	disabled test backward simple script call    main   distautogradtestwithspawn 
module: flaky-tests	disabled test accurracy    main   trainmnist 
module: flaky-tests	disabled test torchscript functions not supported    main   jitrpctestwithspawn 
module: flaky-tests	disabled test stress heavy rpc torchscript    main   rpctestwithspawn 
module: flaky-tests	test dist optim is flaky
module: flaky-tests	test remote script module is flaky
module: flaky-tests	testjitfuser is flaky on windows
module: flaky-tests	test jit fuser is flaky on windows
module: flaky-tests	test backward python udf error is flaky
module: flaky-tests	testcppextensionaot test cuda extension is flaky on windows
module: flaky-tests	testcuda test streams is flaky on circleci windows 
module: flaky-tests	test broadcast coalesced nccl is flaky
module: flaky-tests	test cos scalar cpu in test autograd py  is flaky
module: flaky-tests	test nccl timeouts is flaky
module: flaky-tests	flaky test test cos scalar cpu testautograddevicetypecpu on macos
module: flaky-tests	test handle send exceptions is flaky
module: flaky-tests	test quantized is flaky
module: flaky-tests	test c  d test allgather ops is flaky
module: flaky-tests	test nccl errors blocking clean exit is flaky
module: flaky-tests	test debug info is flaky
module: flaky-tests	testfakequantizeperchannel test backward per channel is flaky
module: flaky-tests	test conv transposed large cuda failed on windows
module: flaky-tests	disabled test get gradients  dist autograd test distautogradjittestwithspawn 
module: flaky-tests	test logdet  x  cuda is flaky
module: flaky-tests	move hypothesis installation to docker
module: flaky-tests	test process group debug info is flaky
module: flaky-tests	testautograddevicetypecuda test logdet  x  cuda is flaky
module: flaky-tests	flaky docker pull on android build ci
module: flaky-tests	test rref context debug info is flaky
module: flaky-tests	disabled test max pool d    main   testquantizedops 
module: flaky-tests	flaky  test constant fold slice    tests on  caffe  onnx py  gcc  ubuntu      test  job
module: flaky-tests	testtorchdevicetypexla  could not start grpc server
module: flaky-tests	test graph for py nested call is flaky
module: flaky-tests	test gloo backend  gpu module is flaky
module: flaky-tests	disabled test async grad guard with grad  jit test async testasync 
module: flaky-tests	test graph for py nested remote call    main   distautogradtestwithspawn  is flaky
module: flaky-tests	rocm ci may hang timeout after testing nnpack
module: flaky-tests	rocm ci may hang after nccl tests are run
module: flaky-tests	nccl ops test py  ncclopstest  test nccl reduce scatter is flaky in rocm ci
module: flaky-tests	nccl ops test py  ncclopstest  test nccl broadcast is flaky in rocm ci
module: flaky-tests	test async parsing  jit test async testasync  segfaults non deterministically
module: flaky-tests	new rocm flakiness
module: flaky-tests	test distributeddataparallel syncbatchnorm diff input sizes running value is flaky
module: flaky-tests	test insert quant dequant jit test is flaky
module: flaky-tests	flaky download of docker in xla
module: flaky-tests	testquantizedops  test max pool d is flaky in ci
module: flaky-tests	rocm ci may hang during nccl ops test py  ncclopstest  test nccl allgather
module: flaky-tests	testquantizedops  test adaptive avg pool d nhwc is flaky in ci
module: flaky-tests	testfakequantizepertensor  test numerical consistency per tensor is failing in ci
module: flaky-tests	rpctestwithspawn test stress heavy rpc is flaky in ci
module: flaky-tests	rocm ci may hang after testdlconvertor
module: flaky-tests	rpctestwithfork test nested rref is flaky in ci
module: flaky-tests	rpctestwithfork test nested rpc is flaky in ci
module: flaky-tests	testscript  test tracing multiple methods is flaky in ci
module: flaky-tests	caffe  onnx ci s testcaffe backendembed opset  test randn like is flaky
module: flaky-tests	caffe  utilsnmstest gpuequalscpucorrectnesstest is flaky in ci
module: flaky-tests	onnx ci s test sh is flaky and can segfault
module: flaky-tests	testqnnpackops test avg pool d is flaky on macos ci test builds
module: flaky-tests	jenkins ci builds sometimes fail in ci with fatal  java nio channels closedchannelexception
module: flaky-tests	test py rref args user share rpc test is flaky
module: flaky-tests	gradle build is flaky   could not get resource 
module: flaky-tests	test multi py udf remote rpc test is flay
module: flaky-tests	windows ci build is flaky  cannot delete workspace    because it is being used by another process
module: flaky-tests	rpctestwithfork test stress light rpc is flaky
module: flaky-tests	testcuda test copy non blocking is flaky on rocm
module: flaky-tests	 tracking issue  rpc tests are flaky
module: flaky-tests	test backward node failure is flaky
module: flaky-tests	conda downloads from anaconda org are flaky sometimes
module: flaky-tests	 parallelworkerstest testparallelworkersinitfun is flaky
module: flaky-tests	test rand quantization is flaky
module: flaky-tests	import torch sometimes fails in ci
module: flaky-tests	ci builds sometimes hang in or just after running test dataloader
module: flaky-tests	rpctest test nested rpc is flaky
module: flaky-tests	utilsnmstest gpuequalscpurotatedcorrectnesstest is flaky
module: flaky-tests	test invalid names is flaky
module: flaky-tests	nightly build failure on windows   convert to int of same size  related 
module: flaky-tests	test conv api    main   moduleapitest  is flaky
module: flaky-tests	test numerical consistency is flaky
module: flaky-tests	test det logdet slogdet batched  in test cuda py  fails on ppc  le
module: flaky-tests	testquantizedops test adaptive avg pool d is flaky
module: flaky-tests	test broadcast ops intermittently segfaults
module: flaky-tests	test nuclear norm axes small brute force is flaky
module: flaky-tests	testjit test cpp flaky
module: flaky-tests	test signal window functions is flaky on rocm
module: flaky-tests	test triangular solve is flaky
module: flaky-tests	awscli install is flaky
module: flaky-tests	runtimeerror  creating mtgp constants failed
module: flaky-tests	test namedtuple return is flakey
module: flaky-tests	test input dict empty list on rocm is flaky
module: flaky-tests	testjit test input dict unify is flaky
module: flaky-tests	distributeddataparalleltest  test dist broadcast coalesced gloo is flaky
module: flaky-tests	fix no sigchld checking in dataloaderiter  shutdown workers
module: flaky-tests	test proper exit is flaky
module: flaky-tests	test proper exit intermittently fails
module: flaky-tests	caffe  test atomic ops segfaulting intermittently
module: flaky-tests	test lstm fusion cpu is flaky
module: flaky-tests	 ppc  le pytorch  random failures in test det logdet slogdet  test torch testtorch 
module: flaky-tests	flaky test  not reentrant test matmul  d  d and test matmul  d  d on os x
module: flaky-tests	test backwards fork flaky test  plus deadlock after 
module: third_party	 v       third party  update pybind to point to fork 
module: third_party	automated submodule update  fbgemm
module: third_party	third party  bump fbgemm to  bb  bf 
module: third_party	distributed using gloo on multiple nodes does not work
module: third_party	update nccl to v       
module: third_party	cannot compile cpp extensions for benchmarking
module: third_party	 build  update googletest
module: third_party	illegal instruction   in      osx   gpu
module: third_party	upgrade mkl dnn to v      
module: third_party	add possibility for miniz to use an external crc definition 
module: third_party	automatic update of fbcode onnx to     e          f bba a cb    cf c eb d  
module: third_party	set miniz no time to avoid computing localtime on each pickle unpickle
module: third_party	work around a gcc   bug in building debug version of sleef
module: third_party	bump gloo
module: third_party	bump gloo
module: third_party	bump gloo
module: third_party	remove unnecessary node  closures in operator registration
module: third_party	do not attempt to add  o  for debug build for sleef 
module: third_party	work around a gcc   bug in building debug version of sleef
module: third_party	autograd  double backwards function for binary cross entropy loss
module: third_party	 onnx  export det
module: third_party	automatic update of fbcode onnx to       bd   cc     b    c        b dd    
module: third_party	cuda    upgrade
module: third_party	add cuda     support
module: third_party	 jit  fix circular deps in loading
module: third_party	upgrade sleef to v      
module: third_party	automatic update of fbcode onnx to ab b     c   f  b f   eb   eef  e c  a  
module: third_party	 pytorch  rename caffe   mobile threadpool to caffe   mobile pthreadpool
module: third_party	 test do not merge  test for cuda    
module: third_party	add logging in constant propagation pass
module: third_party	fix tracing slice select with dynamic inputs
module: third_party	gradient scaling api
module: third_party	move new zeros to core from thp
module: third_party	vectorized complex unary and binary op support 
module: third_party	automatic update of fbcode onnx to   bb ea a  f  e      a   f  bd adb  d   
module: third_party	tensorrt     support and pytorch  onnx  trt  unit test
module: third_party	implement torch nn embedding   embeddingbag in pytorch c   api
module: third_party	automatic update of fbcode onnx to     afc f   f     faa     e    f  bcc b 
module: third_party	automatic update of fbcode onnx to      c adec   e   e     c    ece aa f  f
module: third_party	 c    add nn bilinear to c   frontend
module: third_party	automatic update of fbcode onnx to     d    b  e          e b d aa   db    
module: third_party	upgrade mkldnn to v      
module: third_party	 quantization  factor unnecesary work out of add inner loop
module: third_party	updating submodules
module: third_party	 not for landing  update fbgemm to thread safe codecache
module: third_party	 quantization  vectorized specialization of max pool d for channels last layout
module: third_party	change the method type of generate square subsequent mask
module: third_party	update qnnpack submodule to  d a e 
module: third_party	build torch distributed with gloo backend on macos
module: third_party	 android  init  jni java wrapper for pytorchscript api
module: third_party	 android  init  jni java wrapper for pytorchscript api
module: third_party	jni java wrapper for pytorchscript api
module: third_party	jni java wrapper for pytorchscript api
module: third_party	update qnnpack submodule to    e d 
module: third_party	 not for commit  pytorch android integration into circleci drafting
module: third_party	 quantization  extra repr for quantized modules
module: third_party	 not for commit  testing circleci  pytorch android into circleci drafting
module: third_party	conv d conv d performance on tesla arch   half precision 
module: third_party	 android turn off fbgemm for android build  fixes libtorch android x  
module: third_party	 android  init  jni java wrapper for pytorchscript api
module: third_party	 not for landing  update fbgemm to the latest version with vnni instruction support and latest asmjit version
module: third_party	 jit  make sure nametuples have unique qualified names
module: third_party	 quant  add intrinsic module mappings
module: third_party	updated pixel shuffle in opset    to use depthtospace
module: third_party	 test bump gloo testing
module: third_party	 test  wip  bump gloo test
module: third_party	adds random and fix for dunder config 
module: third_party	port  pow  operator from the th code to aten
module: third_party	 quant  remove qconfig dict from api
module: third_party	automatic update of fbcode onnx to   ca   b  b a        defca       a a    
module: third_party	bump gloo
module: third_party	update nccl to        
module: third_party	 jit  make optimize a thread local flag
module: third_party	update sleef to master  fixes       
module: third_party	automatic update of fbcode onnx to          b    b    b d c aad  d b   d dd
module: third_party	upgrade mkl dnn to v      
module: third_party	op   tests
module: third_party	moving sign function to aten
module: third_party	implements geometric mean initialization
module: third_party	add line numbers to jit log h
module: third_party	update protobuf to v     
module: third_party	update mkldnn bridge to avoid mem leak
module: third_party	automatic update of fbcode onnx to    aa      fa   e  f   cb   ec  ce ddcca
module: third_party	automatic update of fbcode onnx to d  f  d  a a    d        ceaf      f  f 
module: third_party	std opset export 
module: third_party	build error   multiple definition of  omp get place num 
module: third_party	 abandoned  export onnx layernorm
module: third_party	restoring heads for ideep and onnx to more recent versions
module: third_party	fix syncbatchnorm running var update issue
module: third_party	bump gloo
module: third_party	 jit  make compilationunit own functions
module: third_party	automatic update of fbcode onnx to    a    ea e    a e            c  feb b 
module: third_party	porting convtranspose d to aten
module: third_party	update mkldnn bridge to fix mkldnn grouped conv issue
module: third_party	adding liveness test cases back
module: third_party	caffe  mobile opengl         
module: third_party	cleaned codes  added bc tests
module: third_party	automatic update of fbcode onnx to dd   b  f   eb   a  f e      a     dbe e
module: third_party	restore fbgemm version bump 
module: third_party	updating gemmlowp tp  fb c
module: third_party	revert  redefine scheduler to set learning rate using recursive formula        
module: third_party	fix process group for tensors shared across processes
module: third_party	defer constructing error strings for definitions under if s until they re needed 
module: third_party	update fbgemm submodule
module: third_party	automatic update of fbcode onnx to     f ac             f c  e   cd   c bc 
module: third_party	 mkldnn  pooling  support ceil mode by padding changes
module: third_party	implement  cycle learning rate policy
module: third_party	re add tensor t
module: third_party	automatic update of fbcode onnx to           e   f a  da f acf   bc e eb ca
module: third_party	port convtranspose d
module: third_party	porting spatialdilatedconvolution and volumetricdilatedconvolution to aten
module: third_party	 jit  implements divmod function
module: third_party	update submodule url based on redirection 
module: third_party	persist entire git checkout to workspace  don t checkout in subjobs 
module: third_party	remove padding mode from torch nn functional conv       d s docstr
module: third_party	update nccl submodule to v     
module: third_party	use c    list
module: third_party	 wip  test pr to showcase the comparison ops return type change
module: third_party	torch rename
module: third_party	automatic update of fbcode onnx to cc    a f   caca    b        f     dd   
module: third_party	upgrade mkldnn bridge
module: third_party	automatic update of fbcode onnx to ead   a  d   a a a  e ba a  ca e  ec    
module: third_party	update sleef 
module: third_party	restore tbb module
module: third_party	update sleef to include fix for fma  detection
module: third_party	automatic update of fbcode onnx to e  efaa  ed     dfa         c         b 
module: third_party	numpy like nonzero  called nonzero tuple 
module: third_party	automatic update of fbcode onnx to  bde       b        bce  f   d  eda  d e
module: third_party	 testing  is slow test working
module: third_party	use ignore dirty in submodules 
module: third_party	automatic update of fbcode onnx to  d bc  d  a      d e affa c ea b e    ef
module: third_party	implement transpose operator for mkldnn
module: third_party	automatic update of fbcode onnx to f    e  ec a  cbf     cd f     cbf  c   
module: third_party	  cuda     resolve host define h warnings
module: third_party	bump up the foxi submodule
module: third_party	update foxi version
module: third_party	automatic update of fbcode onnx to      bfd dcc baebf  e b   a      f      
module: third_party	 cuda     resolve host define h warnings
module: third_party	wip  port spatialaveragepooling cpu to aten
module: third_party	automatic update of fbcode onnx to   d b   e    cda d d c  ff b  d   f     
module: third_party	automatic update of fbcode onnx to  e d bc e     c  ef   b f  aa   ed bc a 
module: third_party	 testing 
module: third_party	continuation of port max unpool d  max unpool d and max unpool d to aten
module: third_party	 jit  math module support  isnan  asinh  atanh  cosh  sinh  and tanh
module: third_party	adamw and adabound algorithms for c   frontend
module: third_party	batch mode torch cholesky does not raise runtimeerror on gpu if not pos  def 
module: third_party	torch sigmoid behaves inconsistently for     and    bit nan inputs
module: third_party	 pytorch  intel mkl error when doing torch eig on a cuda tensor
module: third_party	torch dot gives strange results 
module: binaries	stuck on pytorch     on macos
module: binaries	docker issue      error  could not find a version that satisfies the requirement torch       
module: binaries	 circleci  remove cuda     binary build jobs
module: binaries	pip install error in python     windows   
module: binaries	homebrew repo is down  https   download pytorch org whl cpu torch stable html
module: binaries	pypi is slow please create git releases
module: binaries	tensor print format issue on nvidia jetson devices
module: binaries	invalid wheel no  dist info   collect env script not working
module: binaries	torch multinomial selects elements with zero weight
module: binaries	i am unable to install  how to install it 
module: binaries	pytorch fails to run on apple silicon
module: binaries	add cuda      
module: binaries	providing a pytorch index conforming to pep    
module: binaries	cannot build pytorch with python     on windows
module: binaries	torch utils collect env return true in is debug build
module: binaries	no kernel image is available for execution on the device
module: binaries	libtorch cpu a is huge    gb  with build android sh
module: binaries	how to build from a release tar package  
module: binaries	cannot import name  use rtld global with libtorch  on when importing torch
module: binaries	pytorch pip installation command for cuda      installs cpu only version
module: binaries	torch       contains avx instruction in the default codepath
module: binaries	installation problems caused by commands provided on the official website
module: binaries	download of torch wheel is very slow
module: binaries	libtorch python so  undefined symbol   py zerostruct
module: binaries	when import torch importerror  libcufft so       cannot open shared object file  no such file or directory
module: binaries	error   torch not compiled with cuda enabled
module: binaries	tensorpipe is shipped as sharedlibrary 
module: binaries	work on pytorch python code without having to build from source
module: binaries	missing aten native cuda headers 
module: binaries	improve error for unsupported python versions when installing with pip
module: binaries	 v       release tracker
module: binaries	thcintegerdivider cuh is not included into pytorch pip nightly builds
module: binaries	complexdoublestorage
module: binaries	py pip installation error zipfile badzipfile  bad crc    for file  torch lib cudnn     dll
module: binaries	setup  add long description to wheel packages
module: binaries	 macos  import torch throws error library not loaded   rpath libc     dylib
module: binaries	pytorch readme build table cleanup for ppc  le
module: binaries	 v       release tracker
module: binaries	libtorch       libiomp  dylib contains erroneous link to  dlc torch libiomp  dylib instead of using  rpath
module: binaries	pytorch     conda package disallows openblas 
module: binaries	pytorch     failed to import c  miniconda  x   envs test lib site packages torch lib caffe  nvrtc dll
module: binaries	macos install error  library not loaded   rpath libc     dylib
module: binaries	 release      circleci  remove python   binary builds
module: binaries	runtimeerror  cuda error  no kernel image is available for execution on the device 
module: binaries	 circleci  change default cuda for pip  cu       cu   
module: binaries	torch version
module: binaries	libtorch version not matching checked out version when building with build libtorch py
module: binaries	torch v      unable to checkout   aaaa    fe a    c f       ba     caad d  in submodule path  third party fbgemm 
module: binaries	no module named tools nnwrap when installing py   torch on windows
module: binaries	cuda for pytorch
module: binaries	c   libtorch zip includes two copies of   so libraries
module: binaries	conda install from pytorch nightly channel with cpu only option delivers       version
module: binaries	cudnn backend not available in nightly           
module: binaries	enable mkl on macos if installed
module: binaries	bizarre  no kernel image  error for pytorch built from source
module: binaries	cudnn status execution failed on python     but not python    
module: binaries	cmake find package returns corrupted library list 
module: binaries	anaconda installation cuda requested  cpuonly obtained
module: binaries	libtorch nightly build is not available on https   pytorch org 
module: binaries	pytorch       doesn t encode numpy dependency
module: binaries	nightly install broken on macos  python    
module: binaries	gpu is not recognized  gcp image  c  deeplearning common cu            
module: binaries	pip torch nightly on macos installs wrong build 
module: binaries	undefined symbol   py zerostruct
module: binaries	should we expose circleci cuda   tests as ximportant and run on prs 
module: binaries	are there any plans to release a prebuilt binary for windows 
module: binaries	assertionerror  torch not compiled with cuda enabled
module: binaries	can t use linux wheel without ld path modifications
module: binaries	old package version in setup py for v     
module: binaries	pytorch night installed from website can not import
module: binaries	installing nightly torch and torchvision shows error
module: binaries	could not find a package configuration file provided by  torch 
module: binaries	how to know which whl version can be selected 
module: binaries	windows release of pytorch     is missing several required cuda libraries
module: binaries	 feature request  provide binaries for python    
module: binaries	put pytorch jni library and fbjni in libtorch distribution 
module: binaries	python   
module: binaries	very slow moving tensor to cuda device  cuda      with pytorch     
module: binaries	website is broken
module: binaries	conda install gives       with cuda    
module: binaries	can not use  cuda   function to load the model into gpu using pytorch    
module: binaries	push based nightlies
module: binaries	pytorch  previous versions  install instructions are confusing
module: binaries	pytorch     on red hat enterprise linux     gives  glibc       not found
module: binaries	can not import pytorch with other library  segmentation fault 
module: binaries	segmentation fault when using with pikepdf
module: binaries	win    no cuda  importerror  dll load failed  the specified module could not be found 
module: binaries	build pytorch with c     abi
module: binaries	pytorch version conflict on colab
module: binaries	install old version pytorch
module: binaries	fft not possible with macos pip binaries
module: binaries	trouble installing pytorch for cuda    
module: binaries	access denied when dowloading libtorch c   without cuda from website
module: binaries	importerror  libcudart so      cannot open shared object file  no such file or directory
module: binaries	can pytorch     support cuda     
module: binaries	pytorch rollback failure
module: binaries	conda installs cpu only version of pytorch nightly during new package uploads  cpu and win   are uploaded first 
module: binaries	 binaries  conda and wheel packages should not ship tests
module: binaries	dedicated channel for pytorch nightlies  no more munging package names
module: binaries	python     conda nightlies don t work
module: binaries	official and nightly wheel structure plan
module: binaries	versioned nightly url for libtorch with cuda      doesn t work
module: binaries	installing pytorch nightly with pip fails
module: binaries	nightly libtorch doesn t contain recent refactoring of generator code
module: binaries	pytorch should provide cuda     binaries 
module: binaries	no pypi packages for python     which will be released soon  expected october 
module: binaries	 pip install   find links  installs package from pypi instead of pytorch org
module: binaries	install only a specific version via pip
module: binaries	official instructions for how to build libtorch don t have same structure as prebuilt binaries
module: binaries	i can t import pytorch  libomp dylib can t be loaded 
module: binaries	 bug  make non default binaries track specific features via x y z feature
module: binaries	 osx  pip package does not ship mkl  runtimeerror  fft  aten not compiled with mkl support
module: binaries	building pytorch master does not install mklml
module: binaries	is mkl dnn enabled in the latest binary distribution v      
module: binaries	libtorch binaries compiled with flag  glibcxx use cxx   abi    
module: binaries	issues to run minimal pytorch example
module: binaries	illegal instruction  core dumped  on debug cpu build
module: binaries	current master doesn t link against nvtoolsext
module: binaries	test binaries are not built with rpath to libnccl so  
module: binaries	        master tracking issue
module: binaries	import torch  libcublas so     error
module: binaries	conda install fails   http     connection failed
module: binaries	    release checklist
module: binaries	runtimeerror  error executing torch shm manager
